{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a551c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "# import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a4fff",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781faac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # 数据文件\n",
    "    train_files = [\n",
    "        '../data/rec_data/train-dataset.csv',\n",
    "        '../data/rec_data/newTrain-dataset.csv',\n",
    "    ]\n",
    "    test_file = '../data/rec_data/test-dataset.csv'\n",
    "    recommend_content_entity_paths = ['/home/zyj/sohu/docker/data/rec_test/recommend_content_entity_0317_初赛.txt',\n",
    "                                     '/home/zyj/sohu/docker/data/rec_test/recommend_content_entity_复赛_训练.txt',\n",
    "                                     '/home/zyj/sohu/docker/data/rec_test/recommend_content_entity_复赛_测试.txt',]\n",
    "    history_len = 15\n",
    "    entity_len = 10\n",
    "    his_entity_len = 20\n",
    "    emtion_feature_path = './tmp/sentiment.dic'\n",
    "    output_dir = './checkpoint/tmp/'\n",
    "    # 模型参数\n",
    "    heads = 4\n",
    "    layers = 4\n",
    "    dim = 128\n",
    "\n",
    "    # 训练参数\n",
    "    device=torch.device('cuda:0')\n",
    "    epochs=3\n",
    "    learning_rate = 1e-4\n",
    "    batch_size=1024\n",
    "    eval_epoch = 1\n",
    "    apex = False\n",
    "    seed=42 \n",
    "\n",
    "    # scheduler参数\n",
    "    scheduler='cosine'                   # ['linear', 'cosine'] # lr scheduler 类型\n",
    "    last_epoch=-1                        # 从第 last_epoch +1 个epoch开始训练\n",
    "    batch_scheduler=True                 # 是否每个step结束后更新 lr scheduler\n",
    "    weight_decay=0.01\n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5                    # 如果使用 cosine lr scheduler， 该参数决定学习率曲线的形状，0.5代表半个cosine曲线\n",
    "\n",
    "    # log参数\n",
    "    log_step = 5\n",
    "    wandb = False\n",
    "    key_metrics = 'auc'\n",
    "\n",
    "\n",
    "    \n",
    "#=======设置全局seed保证结果可复现====\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31588b5d",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae9761",
   "metadata": {},
   "source": [
    "### 处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e5f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(path):\n",
    "    print(f'read {path}')\n",
    "    data = pd.read_csv(path)\n",
    "    data['time'] = pd.to_datetime(data['logTs'],unit='ms',origin=pd.to_datetime('1970-01-01 08:00:00'))\n",
    "    data['Hour'] = data['time'].dt.hour\n",
    "    data['Min'] = data['time'].dt.hour*60+data['time'].dt.minute\n",
    "    data['seq'] = data['userSeq'].str.split('[;:]').fillna(0)\n",
    "    \n",
    "    logTs_min = data.groupby('pvId')['logTs'].min()\n",
    "    logTs_min = pd.DataFrame({\"logTs_min\": logTs_min}).reset_index()\n",
    "    data = pd.merge(data, logTs_min, how='left', on='pvId')\n",
    "    data['logTs_gap'] = (data['logTs'] - data['logTs_min'])/1000\n",
    "    data['logTs_gap'] = data['logTs_gap'].astype(int)\n",
    "    del data['logTs_min']\n",
    "    del data['userSeq']\n",
    "    # print(len(data), len(data.columns))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d82335",
   "metadata": {},
   "source": [
    "### 生成feat_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9404f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_feat_mapper(feature):\n",
    "#     feat_names = ['itemId', 'Hour', 'operator', 'browserType', 'deviceType', 'osType', 'province', 'city', 'logTs_gap']\n",
    "#     ret = {}\n",
    "#     for name in feat_names:\n",
    "#         dic = {'unk':0}\n",
    "#         idx = 1\n",
    "#         for label in feature[name].unique():\n",
    "#             dic[label] = idx\n",
    "#             idx += 1\n",
    "#         ret[name] = dic\n",
    "#     entity_dic = {'unk':0}\n",
    "#     entity_map = {}\n",
    "#     entity_idx = 1\n",
    "#     for path in CFG.recommend_content_entity_paths:\n",
    "#         with open(path) as f:\n",
    "#             for line in f:\n",
    "#                 if len(line.strip()):\n",
    "#                     js = json.loads(line)\n",
    "#                     entity_map[int(js['id'])] = js['entity']\n",
    "#                     for entity in js['entity']:\n",
    "#                         if entity not in entity_dic:\n",
    "#                             entity_dic[entity] = entity_idx\n",
    "#                             entity_idx += 1\n",
    "#     ret['entity'] = entity_dic\n",
    "\n",
    "#     his_logTs_dic = {'unk':0}\n",
    "#     for row in tqdm(feature[['seq', 'logTs']].itertuples()):\n",
    "#         history = row.seq\n",
    "#         if history ==0 or len(history)==0:\n",
    "#             continue\n",
    "#         else:\n",
    "#             assert len(history)%2==0\n",
    "#             his_logTs = []\n",
    "#             for i in range(0,len(history),2):\n",
    "#                 itemId = int(history[i])\n",
    "#                 logTs = int(history[i+1])\n",
    "#                 his_logTs.append(logTs)\n",
    "#                 if itemId not in ret['itemId']:\n",
    "#                     ret['itemId'][itemId] = len(ret['itemId'])\n",
    "#             his_logTs = (row.logTs - np.array(his_logTs))//1000//3600 # 历史到当前query的时间间隔（小时数）\n",
    "#             for each in his_logTs:\n",
    "#                 each = int(each)\n",
    "#                 if each not in his_logTs_dic:\n",
    "#                     his_logTs_dic[each] = len(his_logTs_dic)\n",
    "#     ret['his_logTs'] = his_logTs_dic\n",
    "\n",
    "#     ret['itemId']['noHistory'] = len(ret['itemId']) # 无历史标签\n",
    "    \n",
    "#     ret['emotion'] = {'pad':0, -2:1, -1:2, 0:3, 1:4, 2:5}\n",
    "        \n",
    "#     for k, v in ret.items():\n",
    "#         print(f'name: {k}\\tlen: {len(v)}')\n",
    "#     with open('./tmp/feat_mapper.pkl', 'wb') as f:\n",
    "#         pickle.dump(ret, f)\n",
    "#     print('save feat_mapper at ./tmp/feat_mapper.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486df21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "# for path in CFG.train_files + [CFG.test_file]:\n",
    "#     features.append(read_raw_data(path))\n",
    "# gen_feat_mapper(pd.concat(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fe754",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27c380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, feature:pd.DataFrame=None, feature_name:str=None):\n",
    "        if feature is None:\n",
    "            path = f'./tmp/{feature_name}.csv'\n",
    "            assert os.path.exists(path)\n",
    "            self.df = pd.read_csv(path)\n",
    "        else:\n",
    "            print(f'{feature_name} feature len = {len(feature)}')\n",
    "            df = {}\n",
    "            # 直接特征\n",
    "            with open('./tmp/feat_mapper.pkl', 'rb') as f:\n",
    "                feat_mapper = pickle.load(f)\n",
    "            names = ['itemId', 'Hour', 'operator', 'browserType', 'deviceType', 'osType', 'province', 'city', 'logTs_gap']\n",
    "            for name in names:\n",
    "                df[name] = feature[name].map(feat_mapper[name], na_action=0).values\n",
    "            \n",
    "            emotionDic = {}\n",
    "            with open(CFG.emtion_feature_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    arr = line.strip().split('\\t')\n",
    "                    if arr[0] == 'id': continue\n",
    "                    emotionDic[int(arr[0])] = json.loads(arr[1])\n",
    "            print(len(emotionDic))\n",
    "\n",
    "\n",
    "            \n",
    "            print('deal history feature...')\n",
    "            entitys = []\n",
    "            emotions = []\n",
    "            history_itemId = []\n",
    "            history_logTs = []\n",
    "            history_entitys = []\n",
    "            history_emotions = []\n",
    "            for row in tqdm(feature[['itemId', 'seq', 'logTs']].itertuples()):\n",
    "                # 实体和情感特征\n",
    "                itemId= row.itemId\n",
    "                entity_feature = [0] * CFG.entity_len\n",
    "                emotion_feature = [0] * CFG.entity_len\n",
    "                assert itemId in emotionDic\n",
    "                entity_emotion_dic = emotionDic[itemId]\n",
    "                # 情感强烈的实体排前面\n",
    "                sorted_emotions = sorted([[x,y] for x,y in entity_emotion_dic.items()], key=lambda pair:-abs(pair[1]))\n",
    "                for idx, (x,y) in enumerate(sorted_emotions[:CFG.entity_len]):\n",
    "                    if x in feat_mapper['entity']:\n",
    "                        entity_feature[idx] = feat_mapper['entity'][x] \n",
    "                    else:\n",
    "                        entity_feature[idx] = feat_mapper['entity']['unk'] \n",
    "                    assert y in feat_mapper['emotion']\n",
    "                    emotion_feature[idx] = feat_mapper['emotion'][y] \n",
    "\n",
    "                assert len(entity_feature) == CFG.entity_len\n",
    "                assert len(emotion_feature) == CFG.entity_len\n",
    "                entitys.append(entity_feature)\n",
    "                emotions.append(emotion_feature)\n",
    "\n",
    "                # 历史itmeId和logTs特征\n",
    "                history = row.seq\n",
    "                history_itemId_feature = [0] * CFG.history_len\n",
    "                history_logTs_feature = [0] * CFG.history_len\n",
    "                history_entitys_feature = [0] * CFG.his_entity_len\n",
    "                history_emotions_feature = [0] * CFG.his_entity_len\n",
    "                history_entity_idx = 0\n",
    "                cmp = set(entity_feature)\n",
    "                if history==0 or len(history)==0:\n",
    "                    history_itemId.append(history_itemId_feature)\n",
    "                    history_logTs.append(history_logTs_feature)\n",
    "                    history_entitys.append(history_entitys_feature)\n",
    "                    history_emotions.append(history_emotions_feature)\n",
    "                    continue\n",
    "                else:\n",
    "                    assert len(history)%2==0\n",
    "                    his_itemId = []\n",
    "                    his_logTs = []\n",
    "                    for i in range(0,len(history),2):\n",
    "                        itemId = int(history[i])\n",
    "                        his_itemId.append(itemId)\n",
    "                        logTs = int(history[i+1])\n",
    "                        his_logTs.append(logTs)\n",
    "                    his_logTs = (row.logTs - np.array(his_logTs))//1000//3600 # 历史到当前query的时间间隔（小时数）\n",
    "                    # logTs大的排前面（取最近的历史信息）\n",
    "                    sortedHis = sorted([[x,y] for x,y in zip(his_itemId,his_logTs)], key=lambda pair:-pair[1])\n",
    "                    for idx, (x,y) in enumerate(sortedHis[:CFG.history_len]):\n",
    "                        if x in feat_mapper['itemId']:\n",
    "                            history_itemId_feature[idx] = feat_mapper['itemId'][x] \n",
    "                        else:\n",
    "                            history_itemId_feature[idx] = 0 \n",
    "                        if y in feat_mapper['his_logTs']:\n",
    "                            history_logTs_feature[idx] = feat_mapper['his_logTs'][y] \n",
    "                        else:\n",
    "                            history_logTs_feature[idx] = 0\n",
    "                        if history_entity_idx < CFG.his_entity_len and x in emotionDic:\n",
    "                            for entity, emotion in list(emotionDic[x].items()):\n",
    "                                if entity in feat_mapper['entity'] and feat_mapper['entity'][entity] in cmp:\n",
    "                                    history_entitys_feature[history_entity_idx] = feat_mapper['entity'][entity] \n",
    "                                    history_emotions_feature[history_entity_idx] = feat_mapper['emotion'][emotion] \n",
    "                                    history_entity_idx += 1\n",
    "                                    if history_entity_idx >= CFG.his_entity_len:\n",
    "                                        break\n",
    "\n",
    "                assert len(history_logTs_feature) == CFG.history_len\n",
    "                assert len(history_itemId_feature) == CFG.history_len\n",
    "                history_itemId.append(history_itemId_feature)\n",
    "                history_logTs.append(history_logTs_feature)\n",
    "                history_entitys.append(history_entitys_feature)\n",
    "                history_emotions.append(history_emotions_feature)\n",
    "            df['history_itemId'] = history_itemId\n",
    "            df['history_logTs'] = history_logTs\n",
    "            df['entitys'] = entitys\n",
    "            df['emotions'] = emotions\n",
    "            df['history_entitys'] = history_entitys\n",
    "            df['history_emotions'] = history_emotions\n",
    "\n",
    "            if 'label' in feature.columns:\n",
    "                df['label'] = feature['label'].values\n",
    "            df = pd.DataFrame(df)\n",
    "            print(df.columns)\n",
    "            df.to_csv(f'./tmp/{feature_name}.csv', index=False)\n",
    "            print(f'save at ./tmp/{feature_name}.csv')\n",
    "\n",
    "            \n",
    "            self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature = self.df.iloc[item].to_dict()\n",
    "        for k,v in feature.items():\n",
    "            if isinstance(feature[k], str):\n",
    "                feature[k] = json.loads(v)\n",
    "        return feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd0020",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63532b05",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5c0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        if d_ff == None:\n",
    "            d_ff = d_model * 4\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.gelu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # MultiheadAttention layer\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout)\n",
    "\n",
    "        # FeedForward layer\n",
    "        self.ff = FeedForward(d_model, hidden_dim, dropout=dropout)\n",
    "\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def init_weight(self, module):\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,need_weights=False)[0])\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12591be",
   "metadata": {},
   "source": [
    "### AutoregressiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2986438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveModel(nn.Module):\n",
    "    def __init__(self, dim, heads, layer_num, device, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.simple_feature = ['itemId', 'logTs_gap', 'Hour', 'operator', 'browserType', \n",
    "                                'deviceType', 'osType', 'province', 'city']\n",
    "\n",
    "        with open('./tmp/feat_mapper.pkl', 'rb') as f:\n",
    "            feat_mapper = pickle.load(f)\n",
    "            # for each in feat_mapper:\n",
    "            #     print(each, len(feat_mapper[each]))\n",
    "        self.feat_emb = {\n",
    "            name:nn.Embedding(len(dic), dim).to(device)\n",
    "            for name, dic in feat_mapper.items()\n",
    "        }\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # # Transformers\n",
    "        # self.history_itemId_transformer = nn.Sequential(*[SelfAttentionLayer(dim, heads//2, hidden_dim, dropout)\n",
    "        #                                 for i in range(layer_num//2)])\n",
    "        # self.history_logTs_transformer = nn.Sequential(*[SelfAttentionLayer(dim, heads//2, hidden_dim, dropout)\n",
    "        #                                 for i in range(layer_num//2)])\n",
    "        # self.entity_transformer = nn.Sequential(*[SelfAttentionLayer(dim, heads//2, hidden_dim, dropout)\n",
    "        #                                 for i in range(layer_num//2)])\n",
    "        # self.emotion_transformer = nn.Sequential(*[SelfAttentionLayer(dim, heads//2, hidden_dim, dropout)\n",
    "        #                                 for i in range(layer_num//2)])\n",
    "                                        \n",
    "        self.feature_transformer = nn.Sequential(*[SelfAttentionLayer(dim, heads, hidden_dim, dropout)\n",
    "                                        for i in range(layer_num)])\n",
    "\n",
    "        # # linears\n",
    "        # self.history_linear = nn.Linear(dim*CFG.history_len, dim)\n",
    "        # self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # self.entity_linear = nn.Linear(dim*CFG.entity_len, dim)\n",
    "        # self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        # classify the whole feature\n",
    "        feat_num = len(self.simple_feature) + CFG.entity_len + CFG.history_len + CFG.his_entity_len\n",
    "        self.linear = nn.Linear(dim*feat_num, 2)\n",
    "\n",
    "        # 随机初始化权重\n",
    "        self.apply(self.init_weight)\n",
    "        for emb in self.feat_emb.values():\n",
    "            torch.nn.init.normal_(emb.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def init_weight(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "    def gen_seq_input(self, batch:dict, batch_name:str, mapper_name:str):\n",
    "        inputs = []\n",
    "        for input in batch[batch_name]: # [seq_len, batch_size]\n",
    "            feature_ids = torch.LongTensor(input).to(self.device)\n",
    "            inputs.append(self.feat_emb[mapper_name](feature_ids))\n",
    "        # return torch.stack(inputs) # [seq_len, batch, dim]\n",
    "        return inputs # [seq_len, batch, dim]\n",
    "\n",
    "\n",
    "    def forward(self, batch:dict):\n",
    "        # batch_size = batch['itemId'].size()\n",
    "        feature_seq = []\n",
    "        # 普通特征\n",
    "        for name in self.simple_feature:\n",
    "            feature_ids = torch.LongTensor(batch[name]).to(self.device)\n",
    "            feature_seq.append(self.feat_emb[name](feature_ids)) # [batch, dim]\n",
    "        # 当前文章实体+情感特征\n",
    "        entitys_seq = self.gen_seq_input(batch, 'entitys', 'entity')\n",
    "        emotion_seq = self.gen_seq_input(batch, 'emotions', 'emotion')\n",
    "        for a,b in zip(entitys_seq, emotion_seq): # CFG.entity_len\n",
    "            feature_seq.append((a+b)/2)\n",
    "        # 历史文章+时间特征\n",
    "        his_itemId_seq = self.gen_seq_input(batch, 'history_itemId', 'itemId')\n",
    "        his_logTs_seq = self.gen_seq_input(batch, 'history_logTs', 'his_logTs')\n",
    "        for a,b in zip(his_itemId_seq, his_logTs_seq): # CFG.history_len\n",
    "            feature_seq.append((a+b)/2)\n",
    "        # 历史文章实体+情感特征\n",
    "        his_entitys_seq = self.gen_seq_input(batch, 'history_entitys', 'entity')\n",
    "        his_emotions_seq = self.gen_seq_input(batch, 'history_emotions', 'emotion')\n",
    "        for a,b in zip(his_entitys_seq, his_emotions_seq): # CFG.history_len\n",
    "            feature_seq.append((a+b)/2)\n",
    "\n",
    "        # entitys_feature = self.entity_transformer(entitys_seq) # [seq_len, batch_size, dim]\n",
    "        # emotion_feature = self.emotion_transformer(emotion_seq) # [seq_len, batch_size, dim]\n",
    "        # # [batch_size, seq*dim]\n",
    "        # et_em_feature = entitys_feature.permute(1,0,2).flatten(-2) + \\\n",
    "        #                     emotion_feature.permute(1,0,2).flatten(-2)\n",
    "        # feature_seq.append(self.drop2(self.entity_linear(et_em_feature)))\n",
    "        \n",
    "        # his_itemId_feature = self.history_itemId_transformer(his_itemId_seq) # [seq_len, batch_size, dim]\n",
    "        \n",
    "        # his_logTs_feature = self.history_logTs_transformer(his_logTs_seq) # [seq_len, batch_size, dim]\n",
    "        # his_feature = his_itemId_feature.permute(1,0,2).flatten(-2) + \\\n",
    "        #                     his_logTs_feature.permute(1,0,2).flatten(-2)\n",
    "        # feature_seq.append(self.drop1(self.history_linear(his_feature)))\n",
    "\n",
    "        final_feature = self.feature_transformer(torch.stack(feature_seq)) # [feat_num, batch_size, dim]\n",
    "        out = self.linear(final_feature.permute(1,0,2).flatten(-2)) # [batch_size, 2]\n",
    "\n",
    "        if 'label' in batch:\n",
    "            label = torch.LongTensor(batch['label']).to(self.device)\n",
    "            loss = self.loss(out, label)\n",
    "            return loss, out\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988c44d",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48caaad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(preds, logits, labels): # pre\n",
    "    metrics = {}\n",
    "    metrics['f1'] = f1_score(preds, labels, average='macro')\n",
    "    metrics['auc'] = roc_auc_score(labels, logits[:,1])\n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, valid_dataloader, device):\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    preds = []\n",
    "    logits = []\n",
    "    tk0 = tqdm(enumerate(valid_dataloader),total=len(valid_dataloader))\n",
    "    total_loss = 0\n",
    "    for step, batch in tk0:            \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "                loss, batch_logits = model(batch)\n",
    "        total_loss += loss.item()\n",
    "        labels.append(batch['label'].detach().cpu())\n",
    "        preds.append(batch_logits.argmax(-1).detach().cpu())\n",
    "        logits.append(F.softmax(batch_logits, dim=-1).detach().cpu()) # softmax\n",
    "        \n",
    "    metrics = get_metrics(preds=torch.cat(preds).numpy(),\n",
    "                          logits=torch.cat(logits).numpy(),\n",
    "                          labels=torch.cat(labels).numpy())\n",
    "    metrics['eval_loss'] = total_loss / len(valid_dataloader)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040fe0c",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1aa9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, train_dataloader, valid_dataloader, save_path):\n",
    "    device = CFG.device\n",
    "    best_score = 0\n",
    "    total_step = 0\n",
    "    model.to(device)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    if not len(train_dataloader):\n",
    "        raise EOFError(\"Empty train_dataloader.\")\n",
    "\n",
    "    # 过滤掉冻结的权重\n",
    "    param_optimizer = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    # 设置权重decay\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "    \n",
    "    num_train_steps = int(len(train_dataloader) * CFG.epochs)\n",
    "    if CFG.scheduler=='cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, \n",
    "                    num_warmup_steps=CFG.num_warmup_steps, \n",
    "                    num_training_steps=num_train_steps, \n",
    "                    num_cycles=CFG.num_cycles, \n",
    "#                     last_epoch = ((CFG.last_epoch+1)/CFG.epochs)*num_train_steps\n",
    "                )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "\n",
    "    metrics = evaluate(model, valid_dataloader, device)\n",
    "    print(f\"eval at begin metrics = \")\n",
    "    pprint(metrics)\n",
    "    if CFG.wandb:\n",
    "        wandb.log(metrics, step=total_step)\n",
    "\n",
    "    for cur_epc in range(int(CFG.epochs)):\n",
    "        training_loss = 0\n",
    "        print(\"Epoch: {}\".format(cur_epc))\n",
    "        model.train()\n",
    "        tk0 = tqdm(enumerate(train_dataloader),total=len(train_dataloader))\n",
    "        for step, batch in tk0:\n",
    "            total_step += 1\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "                loss, logits = model(batch)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "            training_loss += loss.item()\n",
    "            tk0.set_postfix(Epoch=cur_epc, Loss=training_loss/(step+1))\n",
    "            if CFG.wandb and (step + 1) % CFG.log_step == 0:\n",
    "                wandb.log({'train_loss':loss, 'lr':optimizer.param_groups[0][\"lr\"], 'epoch': cur_epc},\n",
    "                          step=total_step)\n",
    "        if cur_epc % CFG.eval_epoch == 0:\n",
    "            metrics = evaluate(model, valid_dataloader, device)\n",
    "            print(f\"eval at epoch {cur_epc} metrics = \")\n",
    "            pprint(metrics)\n",
    "            if CFG.wandb:\n",
    "                wandb.log(metrics, step=total_step)\n",
    "            if cur_epc > 0 and metrics[CFG.key_metrics] >= best_score:\n",
    "                best_score = metrics[CFG.key_metrics]\n",
    "                # model_save_path = os.path.join(save_path,f'epoch{cur_epc}.pt') # 保留所有checkpoint\n",
    "                model_save_path = os.path.join(save_path,f'best_checkpoint.pt') # 保留最优checkpoint\n",
    "                torch.save(model, model_save_path)\n",
    "                print(f'save at {model_save_path}')\n",
    "        \n",
    "    torch.cuda.empty_cache()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe84e2",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d645d20",
   "metadata": {},
   "source": [
    "### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306a8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = []\n",
    "# for path in CFG.train_files:\n",
    "#     train_df.append(read_raw_data(path))\n",
    "# train_df = pd.concat(train_df)\n",
    "# # 划分训练测试集\n",
    "# pvId_list = list(set(train_df['pvId']))\n",
    "# random.shuffle(pvId_list)\n",
    "# length = len(pvId_list)\n",
    "# train_pvId = pvId_list[:int(length*0.95)]\n",
    "# valid_pvId = pvId_list[int(length*0.95):]\n",
    "# train = train_df.loc[train_df['pvId'].isin(train_pvId)]\n",
    "# valid = train_df.loc[train_df['pvId'].isin(valid_pvId)]\n",
    "# test = read_raw_data(CFG.test_file)\n",
    "\n",
    "# # 预处理\n",
    "# train_dataset = TrainDataset(train, 'train')\n",
    "# valid_dataset = TrainDataset(valid, 'valid')\n",
    "# test_dataset = TrainDataset(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a37bba",
   "metadata": {},
   "source": [
    "## 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4f1cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964f222968c842d5aa7783ac7fdc4ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval at begin metrics = \n",
      "{'auc': 0.48506335314483695,\n",
      " 'eval_loss': 0.5589232290874828,\n",
      " 'f1': 0.4649077099712953}\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93edca7146404f50b1f6b14d25925181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80f23bb4bd3441cb7137c30bfc05074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval at epoch 0 metrics = \n",
      "{'auc': 0.6171822044691363,\n",
      " 'eval_loss': 0.5225819716670297,\n",
      " 'f1': 0.4481230365087934}\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f89b378fe411b9e4f5be8d6a6c436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf46eb7349c94194b07778efd9191330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval at epoch 1 metrics = \n",
      "{'auc': 0.6225705959561388,\n",
      " 'eval_loss': 0.5110003240541978,\n",
      " 'f1': 0.4496319671878873}\n",
      "save at ./checkpoint/tmp/best_checkpoint.pt\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f2dd2900814ef2a472d1efeab2241a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(seed=42)\n",
    "    if not os.path.exists(CFG.output_dir):\n",
    "        os.makedirs(CFG.output_dir)\n",
    "    with open(os.path.join(CFG.output_dir, 'config.txt'), 'w') as f:\n",
    "        for k,v in CFG.__dict__.items():\n",
    "            f.write(f'{k}: {v}\\n')\n",
    "\n",
    "    # 加载数据\n",
    "    train_dataset = TrainDataset(feature_name='train')\n",
    "    valid_dataset = TrainDataset(feature_name='valid')\n",
    "    test_dataset = TrainDataset(feature_name='test')\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=CFG.batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=CFG.batch_size)\n",
    "\n",
    "    # 加载模型\n",
    "    model = AutoregressiveModel(CFG.dim, CFG.heads, CFG.layers, CFG.device)    \n",
    "    if CFG.wandb:\n",
    "        wandb.init(project='sohu-2022-Transformer推荐', \n",
    "                   name=f'dim{CFG.dim}-head{CFG.heads}-layer{CFG.layers}')\n",
    "    \n",
    "    # 训练\n",
    "    train_eval(model, train_dataloader, valid_dataloader, CFG.output_dir)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68363c32bc78c084bdaff27025a24800f3f94a49f8f5c3bdaad7e3db06aa8762"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deepctr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
