{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc651cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__: 4.5.1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from scipy.special import softmax\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer\n",
    "from utils import sample_context_by_list, bm25_sample\n",
    "# from  dice_loss import  DiceLoss\n",
    "# from  focalloss import  FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeb9f2",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818dd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CFG:\n",
    "    # train_file = '../nlp_data/train.sample.txt'    \n",
    "    # test_file = '../nlp_data/test.txt'\n",
    "    # model=\"/home/yjw/ZYJ_WorkSpace/PTMs/chinese-roberta-wwm-ext/\" \n",
    "    train_file = '../../nlp_data/final/train.mix.txt'    \n",
    "    valid_file = '../../nlp_data/final/valid.mix.txt'    \n",
    "#     train_file = '../../nlp_data/final/train1.txt'    \n",
    "#     valid_file = '../../nlp_data/final/valid1.txt'    \n",
    "    test_file = '../../nlp_data/test.txt'\n",
    "    model=\"/home/zyj/PTMs/chinese-roberta-wwm-ext/\" \n",
    "    output_dir = './roberta-saved'\n",
    "    epochs=5\n",
    "    learning_rate = 2e-5\n",
    "    batch_size=16\n",
    "    max_len=512        \n",
    "    weight_decay=0.01        \n",
    "    seed=42 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059aa976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======设置全局seed保证结果可复现====\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6b681",
   "metadata": {},
   "source": [
    "### mask数据format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e934a",
   "metadata": {},
   "source": [
    "#### prepare_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde7d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPER = {-2:\"坏\",-1:\"差\",0:\"平\",1:\"好\",2:\"棒\"}\n",
    "\n",
    "def prepare_input(content, entitys, labels=None, TOKENIZER=None):\n",
    "    inputs = TOKENIZER(content,add_special_tokens=True,\n",
    "                       truncation = True,\n",
    "                       max_length=CFG.max_len,\n",
    "                       padding=\"max_length\",\n",
    "                       return_offsets_mapping=False)\n",
    "    if labels==None:\n",
    "        pass\n",
    "    else: ## 形成标签inputs\n",
    "        mlm_labels = []\n",
    "        labels_idx = 0\n",
    "        for tk in inputs.input_ids:\n",
    "            if TOKENIZER.convert_ids_to_tokens(tk) == TOKENIZER.mask_token:\n",
    "                label_token = MAPPER[labels[labels_idx]]\n",
    "                mlm_labels.append(TOKENIZER.convert_tokens_to_ids(label_token)) # 加入映射后的prompt token id\n",
    "                labels_idx += 1\n",
    "            else:\n",
    "                mlm_labels.append(-100) # 非mask部分\n",
    "        assert labels_idx==len(labels)\n",
    "        assert len(mlm_labels)==len(inputs.input_ids)\n",
    "        inputs['labels'] = mlm_labels\n",
    "    ret = {}\n",
    "    ret['input_ids'] = torch.tensor(inputs['input_ids'])\n",
    "    ret['attention_mask'] = torch.tensor(inputs['attention_mask'])\n",
    "    ret['labels'] = torch.tensor(inputs['labels'])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663e898",
   "metadata": {},
   "source": [
    "#### format_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250fe009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_line(line, TOKENIZER):\n",
    "    tmp = json.loads(line.strip())\n",
    "    raw_contents = tmp['content'].strip()\n",
    "    if type(tmp['entity']) == dict:\n",
    "        entityArr = list(tmp['entity'].keys())\n",
    "        labels = list(tmp['entity'].values())\n",
    "    elif type(tmp['entity']) == list:\n",
    "        entityArr = tmp['entity']\n",
    "        labels = None\n",
    "    else:\n",
    "        print('entity type error!')\n",
    "    prompt = '在这篇新闻中'\n",
    "    for entity in entityArr:\n",
    "        prompt += f'，{entity}是{TOKENIZER.mask_token}'\n",
    "    prompt += '。'\n",
    "    prompt_token_len = len(TOKENIZER(prompt,add_special_tokens=False).input_ids)\n",
    "    \n",
    "    text = sample_context_by_list(entityArr, raw_contents, length=CFG.max_len-prompt_token_len)\n",
    "    text = prompt + text\n",
    "    \n",
    "    inputs = prepare_input(text, entityArr, labels, TOKENIZER)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806915d",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb2cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_file):\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        self.inputs = []\n",
    "        for line in tqdm(lines):\n",
    "            self.inputs.append(format_line(line.strip(), tokenizer))\n",
    "        print(f'load data from {input_file} len={len(self.inputs)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.inputs[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294b21a",
   "metadata": {},
   "source": [
    "### metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8072e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p) -> dict:\n",
    "    preds,labels=p\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    valid_idx = np.where(labels!=-100)  # 只计算情感标签部分\n",
    "    preds = preds[valid_idx].ravel()\n",
    "    labels = labels[valid_idx].ravel()\n",
    "    print(classification_report(labels, preds))\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f29a62",
   "metadata": {},
   "source": [
    "### 主程序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9124b",
   "metadata": {},
   "source": [
    "#### 加载数据和模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61df6371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6275110313476f81a9dbb82e1c1558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from ../../nlp_data/final/train.mix.txt len=96699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b511bcfd2e4fd490a3a1d1c4e119a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from ../../nlp_data/final/valid.mix.txt len=5789\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainDataset(CFG.train_file)\n",
    "valid_dataset = TrainDataset(CFG.valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92a5e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/zyj/PTMs/chinese-roberta-wwm-ext/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(CFG.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c001ad1",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzyijie\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zyj/sohu/SentimentClassification/mlmModel/wandb/run-20220603_233502-gvgrsu07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/zyijie/sohu-2022-mlm/runs/gvgrsu07\" target=\"_blank\">young-snow-4</a></strong> to <a href=\"https://wandb.ai/zyijie/sohu-2022-mlm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='613' max='30220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  613/30220 06:28 < 5:14:04, 1.57 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EVAL_STEP = len(train_dataset)//CFG.batch_size//3  # 每轮3次\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CFG.output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEP,\n",
    "    save_steps=EVAL_STEP,\n",
    "    save_total_limit=10,\n",
    "#     eval_delay=len(train_dataset)//CFG.batch_size*3,\n",
    "    logging_steps=1,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size,\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    disable_tqdm=False,\n",
    "    eval_accumulation_steps = 5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "wandb.init(project='sohu-2022-mlm')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7da65b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
