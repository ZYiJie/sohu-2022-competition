{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9de3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, BertTokenizer\n",
    "# from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1Score\n",
    "from sklearn.metrics import mean_squared_error, classification_report, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import softmax\n",
    "from scipy import stats\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a81f40",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4aae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex=True\n",
    "    num_workers=0\n",
    "    test_file = '../../nlp_data/test.txt'\n",
    "    base_bert_path = \"/home/zyj/PTMs/ernie-gram-zh/\" \n",
    "    large_bert_path = \"/home/zyj/PTMs/chinese-roberta-wwm-ext-large/\"\n",
    "    classify_model = './newbaseline/roberta-large_all_saved/'\n",
    "    labeling_model = './seqLabeling/roberta-large_len3_random_saved/'\n",
    "#     labeling_model = './seqLabeling/model_saved/allData_roberta-large_batchWeight/'\n",
    "    max_len=512\n",
    "    save_path = './output.txt'\n",
    "\n",
    "    # max_grad_norm=1000  \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# DEVICE = torch.device('cpu') \n",
    "\n",
    "# # 预训练模型目录\n",
    "# base_tokenizer = AutoTokenizer.from_pretrained(CFG.base_bert_path)\n",
    "# # large_tokenizer = AutoTokenizer.from_pretrained(CFG.large_bert_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16f269",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6747a",
   "metadata": {},
   "source": [
    "### SentenceClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58fda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, model_path=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = torch.load(config_path)\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.model.load_state_dict(torch.load(model_path), map_location=torch.device('cuda'))\n",
    "        self.model.cuda()\n",
    "        print(f'load model from {model_path}')\n",
    "        \n",
    "        self.fc = nn.Linear(self.config.hidden_size, 5)\n",
    "        self._init_weights(self.fc)\n",
    "        self.drop1=nn.Dropout(0.1)\n",
    "        self.drop2=nn.Dropout(0.2)\n",
    "        self.drop3=nn.Dropout(0.3)\n",
    "        self.drop4=nn.Dropout(0.4)\n",
    "        self.drop5=nn.Dropout(0.5)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = torch.mean(outputs[0], axis=1)\n",
    "        return last_hidden_states\n",
    "    \n",
    "    def loss(self,logits,labels):\n",
    "\n",
    "        loss_fnc = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([2,1,0.5,1,3])).float() ,\n",
    "                                        size_average=True).cuda()\n",
    "\n",
    "        loss = loss_fnc(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, labels=None, training=True):\n",
    "        feature = self.feature(inputs)\n",
    "        if  training:\n",
    "            logits1 = self.fc(self.drop1(feature))\n",
    "            logits2 = self.fc(self.drop2(feature))\n",
    "            logits3 = self.fc(self.drop3(feature))\n",
    "            logits4 = self.fc(self.drop4(feature))\n",
    "            logits5 = self.fc(self.drop5(feature))\n",
    "            _loss=0\n",
    "            if labels is not None:\n",
    "                loss1 = self.loss(logits1,labels)\n",
    "                loss2 = self.loss(logits2,labels)\n",
    "                loss3 = self.loss(logits3,labels)\n",
    "                loss4 = self.loss(logits4,labels)\n",
    "                loss5 = self.loss(logits5,labels)\n",
    "                _loss = (loss1 + loss2 + loss3 + loss4 + loss5)/5\n",
    "            return _loss\n",
    "        else:\n",
    "            output = self.fc(feature)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            return output\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d790700",
   "metadata": {},
   "source": [
    "### SeqLabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4310271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqLabeling(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.base_bert_path, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            print('='*10+f' load PTM from {cfg.base_bert_path} '+'='*10)\n",
    "            self.model = AutoModel.from_pretrained(cfg.base_bert_path, config=self.config)\n",
    "        else:\n",
    "            print(self.config)\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        self.drop1=nn.Dropout(0.1)\n",
    "        self.drop2=nn.Dropout(0.2)\n",
    "        self.drop3=nn.Dropout(0.3)\n",
    "        self.drop4=nn.Dropout(0.4)\n",
    "        self.drop5=nn.Dropout(0.5)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def loss(self,logits,labels,weights):\n",
    "        # loss_fnc = FocalLoss(6)\n",
    "        # loss_fnc = nn.CrossEntropyLoss()\n",
    "\n",
    "        # loss_fnc = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "        # loss_fnc = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([0.1, 2, 1, 0.5, 1, 3])).float() ,\n",
    "        #                                 size_average=True).cuda()\n",
    "        loss_fnc = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([0.1, 2, 1, 0.5, 1, 3])).float() ,\n",
    "                                        size_average=True,\n",
    "                                        reduction='none').cuda()\n",
    "        # loss_fnc = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([0.1, 2, 1, 0.5, 1, 3])).float() ,\n",
    "        #                                 size_average=True,\n",
    "        #                                 ignore_index=0).cuda()\n",
    "        # loss_fnc = DiceLoss(smooth = 1, square_denominator = True, with_logits = True,  alpha = 0.01 )\n",
    "        loss = loss_fnc(logits, labels)\n",
    "\n",
    "        loss = (loss * weights).mean()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, labels=None, weights=None, training=True):\n",
    "        feature = self.model(**inputs)[0]\n",
    "        # out = self.model(**inputs)\n",
    "        # # first-last-avg\n",
    "        # avg = torch.cat((out.hidden_states[1].unsqueeze(2), \n",
    "        #                  out.hidden_states[-1].unsqueeze(2)), dim=2)                # [batch, seq, 2, emd_dim]\n",
    "        # feature = F.avg_pool2d(avg.transpose(2, 3),kernel_size=(1,2)).squeeze(-1)   # [batch, seq, emd_dim]\n",
    "\n",
    "        if  training:\n",
    "            logits1 = self.fc(self.drop1(feature))\n",
    "            logits2 = self.fc(self.drop2(feature))\n",
    "            logits3 = self.fc(self.drop3(feature))\n",
    "            logits4 = self.fc(self.drop4(feature))\n",
    "            logits5 = self.fc(self.drop5(feature))\n",
    "            _loss=0\n",
    "            if labels is not None:\n",
    "                loss1 = self.loss(logits1.permute(0, 2, 1), labels, weights)\n",
    "                loss2 = self.loss(logits2.permute(0, 2, 1), labels, weights)\n",
    "                loss3 = self.loss(logits3.permute(0, 2, 1), labels, weights)\n",
    "                loss4 = self.loss(logits4.permute(0, 2, 1), labels, weights)\n",
    "                loss5 = self.loss(logits5.permute(0, 2, 1), labels, weights)\n",
    "                _loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n",
    "                # _loss = loss3\n",
    "            return _loss\n",
    "        else:\n",
    "            output = self.fc(feature)\n",
    "            # output = F.softmax(output, dim=1)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793188db",
   "metadata": {},
   "source": [
    "## formater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f08b00",
   "metadata": {},
   "source": [
    "### BM25_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa634524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25_Model(object):\n",
    "    def __init__(self, documents_list, k1=2, k2=1, b=0.75):\n",
    "        self.documents_list = documents_list\n",
    "        self.documents_number = len(documents_list)\n",
    "        self.avg_documents_len = sum([len(document) for document in documents_list]) / self.documents_number\n",
    "        self.f = []\n",
    "        self.idf = {}\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.b = b\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        df = {}\n",
    "        for document in self.documents_list:\n",
    "            temp = {}\n",
    "            for word in document:\n",
    "                temp[word] = temp.get(word, 0) + 1\n",
    "            self.f.append(temp)\n",
    "            for key in temp.keys():\n",
    "                df[key] = df.get(key, 0) + 1\n",
    "        for key, value in df.items():\n",
    "            self.idf[key] = np.log((self.documents_number - value + 0.5) / (value + 0.5))\n",
    "\n",
    "    def get_score(self, index, query):\n",
    "        score = 0.0\n",
    "        document_len = len(self.f[index])\n",
    "        qf = Counter(query)\n",
    "        for q in query:\n",
    "            if q not in self.f[index]:\n",
    "                continue\n",
    "            score += self.idf[q] * (self.f[index][q] * (self.k1 + 1) / (\n",
    "                    self.f[index][q] + self.k1 * (1 - self.b + self.b * document_len / self.avg_documents_len))) * (\n",
    "                             qf[q] * (self.k2 + 1) / (qf[q] + self.k2))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def get_documents_score(self, query):\n",
    "        score_list = []\n",
    "        for i in range(self.documents_number):\n",
    "            score_list.append(self.get_score(i, query))\n",
    "        return score_list\n",
    "    \n",
    "def split_sentence(content:str):\n",
    "    for each in '。；！!?？':\n",
    "        content = content.replace(each, each+'##')\n",
    "    return content.split('##')\n",
    "\n",
    "def bm25_sample(content, query, augment=1, length=512):\n",
    "    \"\"\"\n",
    "    bm25相似度打分，然后进行截断，使其<=length\n",
    "    :param query:\n",
    "    :param content:\n",
    "    :param length:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(content) <= length:\n",
    "        return [content]\n",
    "    else:\n",
    "        document_list = split_sentence(content.strip())\n",
    "        rest_document_list = list()\n",
    "        for document in document_list:\n",
    "            if len(document) != 0:\n",
    "                rest_document_list.append(document)\n",
    "\n",
    "        document_list = rest_document_list\n",
    "        model = BM25_Model(document_list)\n",
    "        scores = model.get_documents_score(query)\n",
    "        index_scores = []\n",
    "        for index, score_i in enumerate(scores):\n",
    "            index_scores.append((index, score_i))\n",
    "\n",
    "        index_scores.sort(key=lambda index_score: index_score[1], reverse=True)\n",
    "        \n",
    "        save_document = [0] * len(document_list)\n",
    "        content_length = 0\n",
    "        \n",
    "        for item in index_scores:\n",
    "            index = item[0]\n",
    "            save_document[index] = 1\n",
    "            if content_length + len(document_list[index]) > length:\n",
    "                break\n",
    "            else:\n",
    "                content_length += len(document_list[index])\n",
    "        if augment ==1: # 不进行数据增强\n",
    "            new_content = \"\"\n",
    "            for i, save in enumerate(save_document):\n",
    "                if save != 0:\n",
    "                    new_content += document_list[i]\n",
    "            # print(len(new_content),'|',new_content)\n",
    "            return [new_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f11297",
   "metadata": {},
   "source": [
    "### sample_context_by_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8331f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_idx(idxArr, span, content):\n",
    "    assert len(idxArr) >= 1\n",
    "    if len(idxArr)==1:\n",
    "        return content[max(0,idxArr[0]-span) : min(len(content),idxArr[0]+span)]\n",
    "    i = 0\n",
    "    ret = []\n",
    "    while True:\n",
    "        if i>=len(idxArr):break\n",
    "        temp_i = i\n",
    "        for j in range(i+1,len(idxArr)):\n",
    "            if idxArr[j]-idxArr[temp_i] > 2*span:\n",
    "                temp_i = j-1\n",
    "                break\n",
    "            else:\n",
    "                temp_i = j\n",
    "        ret.append(content[max(0,idxArr[i]-span) : min(len(content),idxArr[temp_i]+span)])    \n",
    "        i = temp_i+1\n",
    "    return '#'.join(ret)\n",
    "            \n",
    "def sample_context_by_list(entitys:list, content:str, length:int):\n",
    "    '''\n",
    "    通过entity列表筛选content中对应每个实体位置的前后文\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for entity in entitys:\n",
    "        cnt += content.count(entity)\n",
    "    if cnt == 0 or len(content)<=length:\n",
    "        return content\n",
    "    span = int(length/cnt/2)\n",
    "    idxArr = []\n",
    "    for entity in entitys:\n",
    "        idx = content.find(entity,0)\n",
    "        while idx != -1:\n",
    "            idxArr.append(idx)\n",
    "            idx = content.find(entity,idx+1)\n",
    "    idxArr = sorted(idxArr)\n",
    "    result = merge_idx(idxArr, span, content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15706d",
   "metadata": {},
   "source": [
    "### SentenceClassifier Formater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4eda34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_prepare_input(text, feature_text):\n",
    "    inputs = base_tokenizer(text, feature_text, \n",
    "                               add_special_tokens=True,\n",
    "                               truncation = True,\n",
    "                               max_length=CFG.max_len,\n",
    "                               padding=\"max_length\",\n",
    "                               return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "def sc_formater(dic:dict):\n",
    "    ret = []\n",
    "    raw_contents = dic['content']\n",
    "    raw_entitys = dic['entity']\n",
    "    for entity in raw_entitys:\n",
    "        raw_content = raw_contents.strip()\n",
    "        # text = sample_context(entity, text, CFG.max_len-20)\n",
    "        texts = bm25_sample(raw_content, entity, length=CFG.max_len-len(entity))\n",
    "        text = texts[0]\n",
    "        inputs = sc_prepare_input(text, entity)\n",
    "        ret.append(inputs)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9580303",
   "metadata": {},
   "source": [
    "### SeqLabeling Formater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4121d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxtch_token(token_ids:list, sentence_ids:list):\n",
    "    # 得到实体的token list在句子的所有开始位置\n",
    "    ret = []\n",
    "    startId = token_ids[0]\n",
    "    for idx, candId in enumerate(sentence_ids):\n",
    "        if candId == startId and sentence_ids[idx:idx+len(token_ids)] == token_ids:\n",
    "            ret.append(idx)\n",
    "    assert len(ret) > 0\n",
    "    return ret\n",
    "\n",
    "def tag_entity_span(entity_startIndexs:list, entityLen:int, tag:int, targets:list):\n",
    "    for index in entity_startIndexs:\n",
    "        for span in range(entityLen):\n",
    "            targets[index+span] = tag\n",
    "    return targets\n",
    "\n",
    "def getTrainEntityInfo(entityDic, TOKENIZER):\n",
    "    # entityDic{实体名:标签}\n",
    "    tagDic = {}  # {实体名:[实体ids, 实体情感标签, 实体在原字典中的次序, 实体ids长度]}\n",
    "    for idx, (entity,label) in enumerate(entityDic.items()):\n",
    "        entityIds = TOKENIZER(entity).input_ids[1:-1]\n",
    "        tagDic[entity] = [\n",
    "            entityIds,\n",
    "            int(label)+3,  #-2~2 => 1~5\n",
    "            idx+1, len(entityIds)\n",
    "        ]\n",
    "    '''\n",
    "    按实体ids长度从短到长排序，后面标注时若出现嵌套实体，会先标注短实体，然后标注长实体覆盖短实体标签\n",
    "    '''\n",
    "    return sorted(tagDic.items(), key=lambda x:x[1][-1])\n",
    "\n",
    "def getTestEntityInfo(entityArr, TOKENIZER):\n",
    "    # entityArr[实体名]\n",
    "    tagDic = {}  # {实体名:[实体ids, 实体在原字典中的次序, 实体ids长度]}\n",
    "    for idx, entity in enumerate(entityArr):\n",
    "        entityIds = TOKENIZER(entity).input_ids[1:-1]\n",
    "        tagDic[entity] = [\n",
    "            entityIds,\n",
    "            idx+1, len(entityIds)\n",
    "        ]\n",
    "    '''\n",
    "    按实体ids长度从短到长排序，后面标注时若出现嵌套实体，会先标注短实体，然后标注长实体覆盖短实体标签\n",
    "    '''\n",
    "    return sorted(tagDic.items(), key=lambda x:x[1][-1])\n",
    "\n",
    "\n",
    "def SL_formater(entityArr, text, TOKENIZER):\n",
    "    entity_content = '、'.join(entityArr)\n",
    "    text = sample_context_by_list(entityArr, text, CFG.max_len-len(entity_content))\n",
    "    inputs = TOKENIZER(text, entity_content, \n",
    "               add_special_tokens=True,\n",
    "               truncation = True,\n",
    "               max_length=CFG.max_len,\n",
    "               padding=\"max_length\",\n",
    "               return_offsets_mapping=False)\n",
    "\n",
    "    labels_ids = [0] * len(inputs.input_ids)\n",
    "    entityInfoItems = getTestEntityInfo(entityArr, TOKENIZER)\n",
    "    for entity, info in entityInfoItems:\n",
    "#         print(1111, entity, info)\n",
    "        entity_startIndexs = maxtch_token(info[0], inputs.input_ids)\n",
    "        labels_ids = tag_entity_span(entity_startIndexs, info[-1], info[1], labels_ids) # 标注labels_ids序列\n",
    "\n",
    "    assert len(inputs['input_ids']) == len(labels_ids)\n",
    "\n",
    "    # 转换为tensor\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    labels_ids = torch.tensor(labels_ids, dtype=torch.long)\n",
    "\n",
    "    return inputs, labels_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bee519",
   "metadata": {},
   "source": [
    "### MaskSeqLabel Formater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203fd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_SL_formater(entityArr, text, TOKENIZER):\n",
    "    \n",
    "    text = sample_context_by_list(entityArr, text, CFG.max_len)\n",
    "    # 保证每个实体出现在文本中\n",
    "    text = '你对%s怎么看？' % '、'.join(entityArr) + text\n",
    "    \n",
    "    entitys = []\n",
    "    temp = {}\n",
    "    for i,entity in enumerate(entityArr):\n",
    "        key = '[et%d]' % i\n",
    "        entitys.append(key)\n",
    "        temp[entity] = len(entity)\n",
    "    temp = sorted(temp.items(), key=lambda x:-x[1]) # 实体按长度排序，避免长词包含短词的情况\n",
    "\n",
    "    for idx, item in enumerate(temp):\n",
    "        key = '[et%d]' % idx\n",
    "        text = text.replace(item[0], key) # 替换原实体\n",
    "        \n",
    "    inputs = TOKENIZER(text,\n",
    "               add_special_tokens=True,\n",
    "               truncation = True,\n",
    "               max_length=CFG.max_len,\n",
    "               padding=\"max_length\",\n",
    "               return_offsets_mapping=False)\n",
    "\n",
    "    idDic = {}  # label_id\n",
    "    label_ids = []\n",
    "    for idx, entity in enumerate(entitys):\n",
    "        idDic[tokenizer(entity).input_ids[1]] = idx+1\n",
    "    for each in inputs.input_ids:\n",
    "        if each in idDic:\n",
    "            label_ids.append(idDic[each])  \n",
    "        else:\n",
    "            label_ids.append(0) \n",
    "#     print(text)\n",
    "#     print(entitys)\n",
    "#     print(inputs.input_ids)\n",
    "#     en_cnt = 0\n",
    "#     for each in label_ids:\n",
    "#         if each :\n",
    "#             en_cnt += 1\n",
    "#     print(en_cnt, label_ids)\n",
    "    label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    \n",
    "    return inputs, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486a1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973a1a0",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd860e7",
   "metadata": {},
   "source": [
    "### seqLabel模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51ba39db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== load PTM from /home/zyj/PTMs/chinese-roberta-wwm-ext/ ==========\n",
      "=========== load model from ./seqLabeling/final_roberta_epoch16_saved/model_fold0_best.bin ===========\n"
     ]
    }
   ],
   "source": [
    "# CFG.base_bert_path = '/home/zyj/PTMs/albert_chinese_base/'\n",
    "CFG.base_bert_path = '/home/zyj/PTMs/chinese-roberta-wwm-ext/'\n",
    "# CFG.base_bert_path = '/home/zyj/PTMs/ernie-gram-zh/'\n",
    "path1 = './seqLabeling/final_roberta_epoch16_saved/'\n",
    "model = SeqLabeling(CFG, config_path=path1+'config.pth', pretrained=True)\n",
    "model.to(DEVICE)\n",
    "model_path = os.path.join(path1, f\"model_fold0_best.bin\")\n",
    "print(f'=========== load model from {model_path} ===========')\n",
    "model.load_state_dict(torch.load(model_path,map_location=torch.device('cuda')))\n",
    "model.eval()\n",
    "logits_path = path1 + 'valid.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "158e04a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0649beb3f0bf41908b8befbde8586289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyj/.local/lib/python3.7/site-packages/ipykernel_launcher.py:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "tokenizer = BertTokenizer.from_pretrained(CFG.base_bert_path)\n",
    "raw_logits = [] # [样本数, 实体数, 5]\n",
    "with open('./section1.txt', 'w') as fw:\n",
    "    with open('../nlp_data/final/valid.mix.txt', 'r') as f:\n",
    "#     with open('../nlp_data/newTest.txt', 'r') as f:\n",
    "#     with open('../Recommendation/data/rec_data/all_content.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            cnt += 1\n",
    "            dic = json.loads(line.strip())\n",
    "            entityLen = len(dic['entity'])\n",
    "            if type(dic['entity']) == dict:\n",
    "                entityArr = list(dic['entity'].keys())\n",
    "            elif type(dic['entity']) == list:\n",
    "                entityArr = dic['entity']\n",
    "            else:\n",
    "                print('type error!')\n",
    "            index = 0\n",
    "#             print(dic['entity'])\n",
    "            inputs, label_ids = SL_formater(dic['entity'], dic['content'], tokenizer)\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    pred_logits = model(inputs,training=False)\n",
    "            pred_logits = pred_logits.squeeze(0).detach().cpu().numpy()\n",
    "            tmp_result = {}\n",
    "#             print(111, pred_logits.shape,max(label_ids))\n",
    "#             print(111, label_ids)\n",
    "            sample_logits = []  # [样本实体数, 5]\n",
    "            for i in range(1, max(label_ids)+1):\n",
    "                ind = np.where(label_ids==i)\n",
    "                logits = pred_logits[ind]   # [实体字数, 6] \n",
    "                logits = logits[:,1: ]      # [实体字数, 5] \n",
    "                logits = softmax(logits, axis=-1) \n",
    "                \n",
    "                sample_logits.append(np.mean(logits, axis=0))\n",
    "                label = np.mean(logits, axis=0).argmax()-2\n",
    "#                 print(dic['entity'][i], label)\n",
    "                tmp_result[entityArr[i-1]] = label\n",
    "                index += 1\n",
    "            raw_logits.append(sample_logits)\n",
    "#             print(1111, str(dic['id']), tmp_result)\n",
    "            fw.write(str(dic['id']) + '\t' + json.dumps(tmp_result, ensure_ascii=False, cls=NpEncoder) + '\\n')\n",
    "#             if cnt==5:\n",
    "#                 break\n",
    "np.save(logits_path, np.array(raw_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcc9df",
   "metadata": {},
   "source": [
    "### mask SeqLabel模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "986c2d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"/home/zyj/sohu/SentimentClassification/domainAdaption/mask_roberta_saved/epoch16/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21158\n",
      "}\n",
      "\n",
      "=========== load model from ./seqLabeling/mask_final_roberta_epoch16_saved/model_fold0_best.bin ===========\n"
     ]
    }
   ],
   "source": [
    "CFG.base_bert_path = '/home/zyj/PTMs/chinese-roberta-wwm-ext-large/'\n",
    "# path1 = './seqLabeling/model_saved/allData_roberta-large_batchWeight/'\n",
    "# CFG.base_bert_path = '/home/zyj/PTMs/ernie-gram-zh/'\n",
    "path1 = './seqLabeling/mask_final_roberta_epoch16_saved/'\n",
    "model = SeqLabeling(CFG, config_path=path1+'config.pth', pretrained=False)\n",
    "model.to(DEVICE)\n",
    "model_path = os.path.join(path1, f\"model_fold0_best.bin\")\n",
    "print(f'=========== load model from {model_path} ===========')\n",
    "model.load_state_dict(torch.load(model_path,map_location=torch.device('cuda')))\n",
    "model.eval()\n",
    "logits_path = path1 + 'valid.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1099931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词表大小= 21128\n",
      "当前词表大小= 21158\n"
     ]
    }
   ],
   "source": [
    "# 载入预训练模型的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.base_bert_path)\n",
    "\n",
    "print('原始词表大小=', len(tokenizer))\n",
    "characters=[]\n",
    "for i in range(30):\n",
    "    characters.append('[et%d]' % i )\n",
    "tokenizer.add_tokens(characters)\n",
    "print('当前词表大小=',len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53d4d1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758b069b1b7745a5945c7f36a0be43b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyj/.local/lib/python3.7/site-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "raw_logits = [] # [样本数, 实体数, 5]\n",
    "with open('./section1.txt', 'w') as fw:\n",
    "    with open('../nlp_data/final/valid.mix.txt', 'r') as f:\n",
    "#     with open('../nlp_data/newTest.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "#         for line in tqdm(random.sample(lines,1)):\n",
    "        for line in tqdm(lines):\n",
    "            cnt += 1\n",
    "            dic = json.loads(line.strip())\n",
    "            entityLen = len(dic['entity'])\n",
    "            if type(dic['entity']) == dict:\n",
    "                entityArr = list(dic['entity'].keys())\n",
    "            elif type(dic['entity']) == list:\n",
    "                entityArr = dic['entity']\n",
    "            else:\n",
    "                print('type error!')\n",
    "            index = 0\n",
    "#             print(len(dic['content']), dic['content'],'\\n')\n",
    "            inputs, label_ids = mask_SL_formater(dic['entity'], dic['content'], tokenizer)\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    pred_logits = model(inputs,training=False)\n",
    "            pred_logits = pred_logits.squeeze(0).detach().cpu().numpy()\n",
    "            tmp_result = {}\n",
    "#             print(111, pred_logits.shape,max(label_ids))\n",
    "#             print(111, label_ids)\n",
    "            sample_logits = []  # [样本实体数, 5]\n",
    "            for i in range(1, max(label_ids)+1):\n",
    "                ind = np.where(label_ids==i)\n",
    "                logits = pred_logits[ind]   # [实体字数, 6] \n",
    "                logits = logits[:,1: ]      # [实体字数, 5] \n",
    "                logits = softmax(logits, axis=-1) \n",
    "                \n",
    "                sample_logits.append(np.mean(logits, axis=0))\n",
    "                label = np.mean(logits, axis=0).argmax()-2\n",
    "#                 print(dic['entity'][i], label)\n",
    "                tmp_result[entityArr[i-1]] = label\n",
    "                index += 1\n",
    "            raw_logits.append(sample_logits)\n",
    "#             print(1111, str(dic['id']), tmp_result)\n",
    "            fw.write(str(dic['id']) + '\t' + json.dumps(tmp_result, ensure_ascii=False, cls=NpEncoder) + '\\n')\n",
    "#             if cnt==5:\n",
    "#                 break\n",
    "np.save(logits_path, np.array(raw_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ae794",
   "metadata": {},
   "source": [
    "### 计算指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d8c7b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyj/.local/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def loadNP(path):\n",
    "    ret = []\n",
    "    logits = np.load(path,allow_pickle=True)\n",
    "    for each in logits:\n",
    "        ret.append(np.array(each))\n",
    "    return np.array(ret)\n",
    "model_names = ['final_ernie_saved', 'final_ernie_epoch8_saved', 'final_roberta_saved', 'final_roberta_epoch16_saved',\n",
    "              'mask_final_ernie_saved', 'mask_final_ernie_epoch8_saved', 'mask_final_roberta_saved', 'mask_final_roberta_epoch16_saved']\n",
    "single_model_num = len(model_names)\n",
    "logitsArr = []\n",
    "for name in model_names:\n",
    "    path = f'./seqLabeling/{name}/valid.npy'\n",
    "    logitsArr.append(loadNP(path))\n",
    "\n",
    "l1 = 0.5*logitsArr[0]+0.5*logitsArr[3]\n",
    "l2 = 0.5*logitsArr[5]+0.5*logitsArr[7]\n",
    "l3 = 0.9*l1+0.1*l2\n",
    "l4 = 0.8*l1+0.2*l2\n",
    "model_names.append('seqEnsemble1')\n",
    "# model_names.append('seqEnsemble2')\n",
    "model_names.append('maskEnsemble1')\n",
    "# model_names.append('maskEnsemble2')\n",
    "model_names.append('allEnsemble1')\n",
    "model_names.append('allEnsemble2')\n",
    "logitsArr.extend([l1, l2, l3, l4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5288f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_ernie_saved \t 0.7618706372318993\n",
      "final_ernie_epoch8_saved \t 0.7609359471499404\n",
      "final_roberta_saved \t 0.764467757109265\n",
      "final_roberta_epoch16_saved \t 0.7660353209646705\n",
      "mask_final_ernie_saved \t 0.7491064837306081\n",
      "mask_final_ernie_epoch8_saved \t 0.7506124828787996\n",
      "mask_final_roberta_saved \t 0.7317627257159598\n",
      "mask_final_roberta_epoch16_saved \t 0.7400643683468536\n",
      "seqEnsemble1 \t 0.783115017661839\n",
      "maskEnsemble1 \t 0.759238357322033\n",
      "allEnsemble1 \t 0.7844019167626832\n",
      "allEnsemble2 \t 0.7817128350934827\n",
      "单模数量 = 8\n",
      "vote \t 0.7838416106311322\n"
     ]
    }
   ],
   "source": [
    "truths = []\n",
    "data = '../nlp_data/final/valid.mix.txt'\n",
    "with open(data, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        dic = json.loads(line.strip())\n",
    "        truths.extend(list(dic['entity'].values()))\n",
    "\n",
    "for idx, logits in enumerate(logitsArr):\n",
    "    preds = []\n",
    "    for entity_logits in logits:\n",
    "        for logit in entity_logits:\n",
    "            preds.append(logit.argmax()-2)\n",
    "    print(model_names[idx], '\\t', f1_score(truths, preds,average='macro'))\n",
    "#     print(classification_report(truths, preds))\n",
    "votes = []\n",
    "print('单模数量 =', single_model_num)\n",
    "for i in range(len(logitsArr[0])):\n",
    "    for j in range(len(logitsArr[0][i])):\n",
    "        temp = []\n",
    "        for k in range(single_model_num):\n",
    "            label = logitsArr[k][i][j].argmax()-2\n",
    "            temp.append(label)\n",
    "        retult = stats.mode(temp)[0][0]\n",
    "        votes.append(retult)\n",
    "print('vote', '\\t', f1_score(truths, votes,average='macro'))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae4ac0",
   "metadata": {},
   "source": [
    "### Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6388f038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"id\": 6000001, \"content\": \"看来大家对于《余生请多指教》还是信心...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{\"id\": 6000002, \"content\": \"阳性感染者92（女，29岁），后经市...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{\"id\": 6000003, \"content\": \"为进一步完善反洗钱监管制度，提高反洗...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{\"id\": 6000004, \"content\": \"1、锅中水烧开，放入面条，煮至九成熟...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{\"id\": 6000005, \"content\": \"导语：2022年，翻身财运旺，好运随...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13288</th>\n",
       "      <td>13288</td>\n",
       "      <td>{\"id\": 6013289, \"content\": \"生肖羊的人外表会感觉忠厚老实，其实他...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13289</th>\n",
       "      <td>13289</td>\n",
       "      <td>{\"id\": 6013290, \"content\": \"韩国女团APink组合的女星孙娜恩在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13290</th>\n",
       "      <td>13290</td>\n",
       "      <td>{\"id\": 6013291, \"content\": \"我现在是不是应该多给他鼓励，不要逼他...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13291</th>\n",
       "      <td>13291</td>\n",
       "      <td>{\"id\": 6013292, \"content\": \"8、一个人要赢得另一个人很容易，那就...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13292</th>\n",
       "      <td>13292</td>\n",
       "      <td>{\"id\": 6013293, \"content\": \"结合近期a股和近两个交易日的实际行情...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13293 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                            content\n",
       "0          0  {\"id\": 6000001, \"content\": \"看来大家对于《余生请多指教》还是信心...\n",
       "1          1  {\"id\": 6000002, \"content\": \"阳性感染者92（女，29岁），后经市...\n",
       "2          2  {\"id\": 6000003, \"content\": \"为进一步完善反洗钱监管制度，提高反洗...\n",
       "3          3  {\"id\": 6000004, \"content\": \"1、锅中水烧开，放入面条，煮至九成熟...\n",
       "4          4  {\"id\": 6000005, \"content\": \"导语：2022年，翻身财运旺，好运随...\n",
       "...      ...                                                ...\n",
       "13288  13288  {\"id\": 6013289, \"content\": \"生肖羊的人外表会感觉忠厚老实，其实他...\n",
       "13289  13289  {\"id\": 6013290, \"content\": \"韩国女团APink组合的女星孙娜恩在...\n",
       "13290  13290  {\"id\": 6013291, \"content\": \"我现在是不是应该多给他鼓励，不要逼他...\n",
       "13291  13291  {\"id\": 6013292, \"content\": \"8、一个人要赢得另一个人很容易，那就...\n",
       "13292  13292  {\"id\": 6013293, \"content\": \"结合近期a股和近两个交易日的实际行情...\n",
       "\n",
       "[13293 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '../nlp_data/newTrain.txt'\n",
    "lines = []\n",
    "idx = 0\n",
    "with open(data, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        lines.append([idx, line])\n",
    "        idx += 1\n",
    "lines  = pd.DataFrame(lines, columns=['idx', 'content'])\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "29fb7d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "问及拥有38000名成员的山口组为何出现在许老的葬礼上，山口组则表示：“我们是来表达对蚊哥（许海清）的尊敬。”山口组在日本横行，在许海清面前却还是很恭敬的自称“小弟”。\n",
      "{'许海清': 0}\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "457\n",
      "孙卓的生日可谓是意义非凡，自从成为了打拐明星后，孙海洋一家的动态就格外受到网友们的关注，孙海洋和孙妈妈也很乐于分享家里的快乐，但很多时候，网友们过分的热情给一家人也带来了不小的困扰。近期，有网友因孙海洋一家不开直播打赏不带货，选择直接用转账的方式对孙海洋一家进行打赏，希望孙海洋能够快点换上一个大房子，给三个孩子更优质的生活。在陆续收到网友们的转账后，孙海洋吓得连夜紧急关闭转账功能。其实这段时间对于孙海洋来说，还有很多东西需要进行整理，官司还没开始，想要胜诉多年积累的资料还需要进一步的归纳提炼，这毕竟会是一场持久战。这次直播中，孙海洋回应孙卓在学校很适应，但现在对孙卓的态度已经有了360度的转变，不光给孙卓起了一个新的称呼，从原来的卓总到卓卓，小卓，现在已经成了憨卓，恭喜卓总喜提新称号。甚至搬家的新房中，只有他的屋没有空调，家庭地位一落千丈，可以见得孙卓现在是真的适应了。但孙海洋也是真的飘了，拿起了一家之主的风范，程孙卓现在一切都好，就是有点小的问题，治一下就好了，所有的问题就是孙卓现在还不喜欢做家务。\n",
      "{'孙海洋': 0, '孙卓': 0}\n",
      "[-1, -1]\n",
      "[-1, 0]\n",
      "[-1, 0]\n",
      "418\n",
      "2010年，姜文投资1.3亿拍《让子弹飞》，妻子周韵向他推荐祖峰，但姜文却不同意：他没有流量，找他演商业片，是要赔钱的。 面对姜文的拒绝，周韵依然不放弃，还是坚持想让祖峰来出演：我和他有过合作，他的演技我很清楚，是非常不错的，要不你再考虑考虑。 是的，周韵曾和祖峰合作过电视剧《金婚风雨情》，两人也因此相识，祖峰精湛的演技给周韵留下了深刻的印象。 所以在姜文筹拍《让子弹飞》时，周韵首先想到了祖峰，并向丈夫力荐他。站在周韵的角度，她首先考虑的是演技，但作为导演的姜文却首要考虑的是片子能不能赚到钱，演员有没有票房号召力。 最终姜文还是选择了既有实力又有流量的周润发和葛优，祖峰知道后，沮丧地称：感觉就像是失恋了。 其实并不是祖峰不好，只能说是姜文不敢轻易冒这个险，来启用祖峰。 在当下演艺圈，祖峰是大器晚成的演员，也是以精湛的演技和低调的品行著称的实力派演员。 在进入演艺圈以前，祖峰曾在汽车制造厂工作过四年，但他却始终对表演情有独钟。\n",
      "{'祖峰': 2, '周润发': 1, '葛优': 1, '姜文': 0, '周韵': 0}\n",
      "[2, 2, 1, 0, 0]\n",
      "[1, 1, 1, 0, 0]\n",
      "[2, 1, 1, 0, 0]\n",
      "355\n",
      "利物浦巧取迪亚斯干得漂亮！热刺列维输给利物浦心有不甘，宁可被曼城豪夺！媒体方面消息，热刺在和波尔图谈判路易斯迪亚斯的过程中，利物浦提前获得了消息，一番运作之后，迪亚斯穿上了利物浦球衣。热刺忙碌多时，却为人做了嫁衣。朱家麦子李家磨，做成一个大馍馍，送给隔壁赵大哥。为富不仁，这个落不到利物浦头上，利物浦是出了名的扣扣索索。当然热刺也不是土豪，家里没有多少余粮。路易斯迪亚斯，被利物浦盯上很久了。这次明显是知彼知己的结果。波尔图需要现金周转，而利物浦能够迅速满足波尔图的要求。路易斯迪亚斯本人，当然选择光环更多的利物浦。不仅仅是欧冠和英超奖杯在手，还有克洛普的个人影响力。孔蒂也有英超和意甲冠军，但是出名的暴脾气和固执，迪亚斯也担心和孔蒂不好相处吧。热刺，哑巴吃黄连，有苦说不出。宁可输给曼城，也不情愿输给利物浦。\n",
      "{'利物浦': -1, '孔蒂': -1, '热刺': 0, '路易斯迪亚斯': 0}\n",
      "[1, -1, -1, 0]\n",
      "[1, -1, 0, 0]\n",
      "[1, -1, -1, 0]\n",
      "445\n",
      "贝祖诒的儿子就是大名鼎鼎的贝聿铭了。由于父亲移居美国，贝聿铭得以在美国长大，并且逐渐成为了世界级别的建筑大师。贝祖诒的原配去世之后，又娶了著名的江南名媛蒋四小姐。贝聿铭还有一个弟弟贝聿昆也是毕业于麻省理工学院，他是全球最权威的玻璃跟陶瓷的专家。贝润生靠着开颜料厂赚到了大钱，一生更是做了无数的慈善事业。贝润生曾经花巨资在苏州建了一座苏州园林狮子林。这座宅子堪称是经典中的经典。中国园林建筑是追求建筑跟自然和谐的极致，这些灵感给了贝聿铭很厚的灵感。小时候贝聿铭也常常在狮子林中玩耍。另外贝润生在园子里面建立了祠堂跟义庄，贝家在苏州常常捐钱捐药，济世救人。贝润生还曾经出资建立了中国第一个新式的幼稚园，极大推动了中国幼儿事业的发展。贝润生的女儿出嫁的时候，他曾经把当时有着远东第一豪宅之称的绿房子送给了女婿。这座绿房子是匈牙利的大师设计的，单单式厨房就有300平方，另外还有12间颜色不一样的厕所。当时豪宅里面还配备了全球最先进的电梯。贝润生在上世纪四五十年代的时候，在上海拥有超过千套的房产。\n",
      "{'贝聿铭': 2, '贝聿昆': 2, '贝润生': 1, '中国': 0, '苏州': 0}\n",
      "[1, 2, 2, 0, 0]\n",
      "[1, 1, 1, 0, 0]\n",
      "[1, 2, 1, 0, 0]\n",
      "478\n",
      "当时泗州失利后，身在南原的王士琦手下都纷纷劝他撤退，但是王士琦坚定地认为若是打仗的监军都走了军队还能有勇气吗，于是众人重整旗鼓打了回去收复了泗州。王士琦把他的军事天赋完全表现了出来，利用地形突袭，诱敌深入后火烧粟林，在最终决战的南海上时，王士琦算准了敌人会突进到阵型中来，一手火烧连营在海面把明朝必胜之师的威武展现得淋漓尽致。据《章安王氏宗谱》记载，王士琦大军后来后“士女老幼争以壶浆劳师”，而且朝鲜人为了表达对王士琦的敬仰还在全州立下了《去思碑》作为纪念。为何王士琦会有大量的墓葬珍宝呢王士琦为官清廉，又多次在战场上体现了爱国的情怀，那么他到底是不是一个真正的清官呢？答案是他真是一个清官。现在的临海市区仍然有一个地方叫“十伞巷”，这里是当年王士琦的父亲王宗沐的故居，当年王宗沐膝下三子士崧，士琦和士业，除了王士业是贡生之外其他三人都是进士，而且王宗沐和二子王士琦，三子王士业都当过巡抚。所以当地的临海百姓称王家是“父子四进士，一门三巡抚”，因为王士琦家族成员当官期间都是清正廉明，所以百姓们为了感谢就给他们送了十把“万民伞”，所以也因此得名“十伞巷”。\n",
      "{'泗州': -1, '王士琦': 1, '王宗沐': 1, '临海': 0}\n",
      "[-1, 1, 0, 0]\n",
      "[1, 1, 1, 0]\n",
      "[-1, 1, 0, 0]\n",
      "439\n",
      "在这个美颜PS横行的时代，除了见到本人。否则一切照片都是“照骗”，明星也不例外！周迅“周公子”一直是她的灵气别具一格，也是实力派演员，演技一直非常在线，在最近热播的《小敏家》，她的状态依然很好！她的生图除了有点鱼尾纹，其他都很好，毕竟她也奔五了！不得不说年轻时候的周迅美的如诗如画！张柏芝这张生图确实有点显老，鱼尾纹明显，整个人感觉又老又憔悴，看来独自一人带大三个儿子实属不易！刚出道的她，美貌被称为“惊为天人”，现在看来的确如此！贾静雯一直被称为“冻龄女神”，生图下的皱纹还是很明显，毕竟已经是三个孩子的妈妈，经历过离婚，好在现在很幸福！曾经的赵敏美的让人过目不忘！杨幂虽然出道十多年，但是也才三十多岁，皱纹这么明显，看来大幂幂不能一直做“拼命三郎”，要注意保养！《三生三世十里桃花》以后就没口碑更好的剧，期待杨幂很多优质作品！据说杨颖的烟瘾很大，之前还被粉丝爆出公众场所抽烟画面，作为三十加的女星有点显老啊！下面这几个，你们觉得和年龄相符吗？#虎年有娱##明星生图##明星#\n",
      "{'周迅': 1, '小敏家': 1, '张柏芝': -1, '贾静雯': -1, '赵敏': 1, '杨幂': -1, '杨颖': -1, '明星': 0, '三生三世十里桃花': 0}\n",
      "[2, 1, -1, 1, 1, 1, -1, 0, 0]\n",
      "[1, 1, -1, 1, -1, -1, -1, 0, 0]\n",
      "[2, 1, -1, 1, 1, 1, -1, 0, 0]\n",
      "495\n",
      "Faye Wong出生于69年。今年是五十岁的孩子，但王维的身体是非常好的，因为它也是高度的高度，而Faye Wong的官方高度是一米，但体重只有九十。金，所以如何穿，加fAYE黄一直是一条特殊的线，非常个人，不小心别人看到自己，想一想只要你活着，你怎么看？事实上，我羡慕这样的王菲，我觉得我还活着，我会过着我想要的东西。王飞的身体一直保持扁平，所以它似乎更冷，有点距离，不容易接近。看看五十岁的福伊黄的身体，然后看看四十岁的Cecilia Cheung，粉丝说尼古拉斯TSE正在思考？ Cecilia Cheung是八年的一年。今年是四十岁，现在它是三个孩子的母亲。但是不要说塞西莉亚张，也就是说，身体是一种祝福，但后来在很短的时间内，它很好，身体恢复特别快。在这个数字恢复之后，她是一个焦点。当张博智时，他过去了，这是不稳定的，这么多年，虽然多年留下了她脸上的痕迹，但Cecilia Cheung的气质更好，而且它真的有些女性。。刚看到了五十岁的王菲的身体，然后看着四十岁的塞西莉亚祥的身体，粉丝说，尼古拉斯蒂斯正在思考，所以每个人都觉得王飞的身体是好的或cecilia cheung？\n",
      "{'王菲': 2, '塞西莉亚张': 0, '尼古拉斯蒂斯': 0}\n",
      "[1, 1, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "528\n",
      "如果提到台湾第一美女，大家肯定首先想到的就是萧蔷，曾经出演过《小李飞刀》里的林诗音，《一帘幽梦》里的汪绿萍，萧蔷的古装扮相简直是美若天仙。若说起知性的代表，首当其冲肯定是杨澜，我们听杨澜的访谈，看她的散文，会觉得她是柔弱温和的。她的学识和教养让她谦逊平和。能将萧蔷那种摄人心魄的美和杨澜知性的力量结合在一起的女人，大概就只有林青霞了。1、40岁以前刚柔并济的林青霞，现在优雅知性的老去徐克说林青霞是“俊的”，这等美人要50年才会出一个，这“俊”让林青霞成功地在38岁时以“东方不败”成功登上第三个事业高峰，这“俊”也让这样的美变得格外经得起时间的打磨，就算是息影了20年，当了20年的家庭主妇，就算是最低调的深蓝，这个女人也有本事把它穿的倜傥无比。林青霞的美在于美而不自知，美到惊艳了时光，她身上有着少有的气质之美，和世间庸俗都沾不上边，早已深印在一代人的心中脸，她更有一身旁人不可企及的脱俗气质，曾是言情大师的琼瑶认定的第一女主角，也是各大导演朝思暮想的缪斯女神，横跨70-90年代，成就一个又一个票房奇迹，用中国老话说“这就是老天赏饭吃”。她的美丽中透着一种洒脱的英气，这英气中又饱含着无尽的温柔，刚柔并济，一瞥一笑如沐春风，一举手一投足皆是风情万种。\n",
      "{'萧蔷': 1, '杨澜': 1, '林青霞': 1, '琼瑶': 1}\n",
      "[2, 2, 2, 0]\n",
      "[2, 1, 1, 0]\n",
      "[2, 2, 1, 0]\n",
      "363\n",
      "在对兰斯一战，梅西替补出场，迎来了2022年的首场比赛。然而，他再一次未能破门，12场法甲只打入1球。《太阳报》的数据显示，梅西本赛季联赛44次射门只打入1球，射门转化率在五大联赛球员里排名倒数第二位。在本赛季，梅西为巴黎打入6球，其中在法甲只打入1球。不仅如此，梅西在12场法甲里已经有44次射门，44次射门只打入1球的效率，让他在五大联赛里排名倒数第二位。在五大联赛球员里，唯一一位效率比梅西更差的球员是曼城后卫坎塞洛，他46次射门只打入1球。不过，在对兰斯一战，梅西送出了本赛季的第6次助攻。在法甲，梅西打入1球，排名射手榜第11位，送出4次助攻则排名法甲第17位。在法甲，梅西场均参与0.42球。尽管受到了转会、伤病、感染新冠病毒等因素的影响，但对梅西来说，这样的效率还是令人惊讶。如何尽快找到状态，是梅西的当务之急。\n",
      "{'坎塞洛': -1, '梅西': -1, '联赛': 0, '球员': 0}\n",
      "[-1, 1, 0, 0]\n",
      "[-1, -1, 0, 0]\n",
      "[-1, -1, 0, 0]\n",
      "32 12\n",
      "29 9\n"
     ]
    }
   ],
   "source": [
    "change1 = 0\n",
    "gaizheng1 = 0\n",
    "change2 = 0\n",
    "gaizheng2 = 0\n",
    "for item in lines.sample(100).itertuples():\n",
    "# for item in lines.itertuples():\n",
    "    dic = json.loads(item.content.strip())\n",
    "    idx = item.idx\n",
    "#     print(dic)\n",
    "    id = dic['id']\n",
    "    tempA = []\n",
    "    tempB = []\n",
    "    tempC = []\n",
    "    flag = False\n",
    "    for i,(entity,label) in enumerate(dic['entity'].items()):\n",
    "        a = logits1[idx][i].argmax()-2\n",
    "        b = logits2[idx][i].argmax()-2\n",
    "        c = (0.7*logits1[idx][i]+0.3*logits2[idx][i]).argmax()-2\n",
    "        tempA.append(a)\n",
    "        tempB.append(b)\n",
    "        tempC.append(c)\n",
    "        if a != label:\n",
    "            change1 += 1\n",
    "            if b == label:\n",
    "                gaizheng1 += 1\n",
    "        if b != label:\n",
    "            change2 += 1\n",
    "            if a == label:\n",
    "                gaizheng2 += 1\n",
    "        if abs(a-label)>=1 and abs(b-label)==0:\n",
    "            flag = True\n",
    "    if flag :\n",
    "        print(len(dic['content']), dic['content'], dic['entity'], tempA, tempB, tempC,sep='\\n')\n",
    "            \n",
    "print(change1, gaizheng1)\n",
    "print(change2, gaizheng2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93107a91",
   "metadata": {},
   "source": [
    "## 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8ad6f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d323314eb3f454eb6de4a7ebf08f26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./section1.txt', 'w') as fw:\n",
    "    fw.write(\"id\tresult\\n\")\n",
    "    with open('../nlp_data/newTest.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(tqdm(lines)):\n",
    "            dic = json.loads(line.strip())\n",
    "            entityLen = len(dic['entity'])\n",
    "            tmp_result = {}\n",
    "            for j in range(entityLen):\n",
    "                label = l3[i][j].argmax()-2\n",
    "                tmp_result[dic['entity'][j]] = label\n",
    "#             print(dic)\n",
    "#             print(tmp_result)\n",
    "            fw.write(str(dic['id']) + '\t' + json.dumps(tmp_result, ensure_ascii=False, cls=NpEncoder) + '\\n')\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3601c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
