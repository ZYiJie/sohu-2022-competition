{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece243f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, BertTokenizer\n",
    "from sklearn.metrics import mean_squared_error, classification_report, f1_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, get_feature_names, VarLenSparseFeat, DenseFeat\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43735773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex=True\n",
    "    num_workers=0\n",
    "    test_file = '../nlp_data/newTest.txt'\n",
    "    seq_model1 = \"./nlp_final_model/final_ernie_saved/\" \n",
    "    seq_model2 = \"./nlp_final_model/final_roberta_epoch16_saved/\" \n",
    "    mask_model1 = \"./nlp_final_model/mask_final_ernie_epoch8_saved/\" \n",
    "    mask_model2 = \"./nlp_final_model/mask_final_roberta_epoch16_saved/\" \n",
    "    max_len=512\n",
    "    \n",
    "    # rec\n",
    "    rec_test = '../Recommendation/data/rec_data/newTest-dataset.csv'\n",
    "    rec_model = './DIFM.h5'\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "device = 'cuda'\n",
    "# DEVICE = torch.device('cpu') \n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91647a93",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c16ba",
   "metadata": {},
   "source": [
    "## sample_context_by_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4197d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_idx(idxArr, span, content):\n",
    "    assert len(idxArr) >= 1\n",
    "    if len(idxArr)==1:\n",
    "        return content[max(0,idxArr[0]-span) : min(len(content),idxArr[0]+span)]\n",
    "    i = 0\n",
    "    ret = []\n",
    "    while True:\n",
    "        if i>=len(idxArr):break\n",
    "        temp_i = i\n",
    "        for j in range(i+1,len(idxArr)):\n",
    "            if idxArr[j]-idxArr[temp_i] > 2*span:\n",
    "                temp_i = j-1\n",
    "                break\n",
    "            else:\n",
    "                temp_i = j\n",
    "        ret.append(content[max(0,idxArr[i]-span) : min(len(content),idxArr[temp_i]+span)])    \n",
    "        i = temp_i+1\n",
    "    return '#'.join(ret)\n",
    "            \n",
    "def sample_context_by_list(entitys:list, content:str, length:int):\n",
    "    '''\n",
    "    通过entity列表筛选content中对应每个实体位置的前后文\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for entity in entitys:\n",
    "        cnt += content.count(entity)\n",
    "    if cnt == 0 or len(content)<=length:\n",
    "        return content\n",
    "    span = int(length/cnt/2)\n",
    "    idxArr = []\n",
    "    for entity in entitys:\n",
    "        idx = content.find(entity,0)\n",
    "        while idx != -1:\n",
    "            idxArr.append(idx)\n",
    "            idx = content.find(entity,idx+1)\n",
    "    idxArr = sorted(idxArr)\n",
    "    result = merge_idx(idxArr, span, content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f00810",
   "metadata": {},
   "source": [
    "## SeqLabeling Formater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b780940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxtch_token(token_ids:list, sentence_ids:list):\n",
    "    # 得到实体的token list在句子的所有开始位置\n",
    "    ret = []\n",
    "    startId = token_ids[0]\n",
    "    for idx, candId in enumerate(sentence_ids):\n",
    "        if candId == startId and sentence_ids[idx:idx+len(token_ids)] == token_ids:\n",
    "            ret.append(idx)\n",
    "    assert len(ret) > 0\n",
    "    return ret\n",
    "\n",
    "def tag_entity_span(entity_startIndexs:list, entityLen:int, tag:int, targets:list):\n",
    "    for index in entity_startIndexs:\n",
    "        for span in range(entityLen):\n",
    "            targets[index+span] = tag\n",
    "    return targets\n",
    "\n",
    "def getTrainEntityInfo(entityDic, TOKENIZER):\n",
    "    # entityDic{实体名:标签}\n",
    "    tagDic = {}  # {实体名:[实体ids, 实体情感标签, 实体在原字典中的次序, 实体ids长度]}\n",
    "    for idx, (entity,label) in enumerate(entityDic.items()):\n",
    "        entityIds = TOKENIZER(entity).input_ids[1:-1]\n",
    "        tagDic[entity] = [\n",
    "            entityIds,\n",
    "            int(label)+3,  #-2~2 => 1~5\n",
    "            idx+1, len(entityIds)\n",
    "        ]\n",
    "    '''\n",
    "    按实体ids长度从短到长排序，后面标注时若出现嵌套实体，会先标注短实体，然后标注长实体覆盖短实体标签\n",
    "    '''\n",
    "    return sorted(tagDic.items(), key=lambda x:x[1][-1])\n",
    "\n",
    "def getTestEntityInfo(entityArr, TOKENIZER):\n",
    "    # entityArr[实体名]\n",
    "    tagDic = {}  # {实体名:[实体ids, 实体在原字典中的次序, 实体ids长度]}\n",
    "    for idx, entity in enumerate(entityArr):\n",
    "        entityIds = TOKENIZER(entity).input_ids[1:-1]\n",
    "        tagDic[entity] = [\n",
    "            entityIds,\n",
    "            idx+1, len(entityIds)\n",
    "        ]\n",
    "    '''\n",
    "    按实体ids长度从短到长排序，后面标注时若出现嵌套实体，会先标注短实体，然后标注长实体覆盖短实体标签\n",
    "    '''\n",
    "    return sorted(tagDic.items(), key=lambda x:x[1][-1])\n",
    "\n",
    "\n",
    "def SL_formater(entityArr, text, TOKENIZER):\n",
    "    entity_content = '、'.join(entityArr)\n",
    "    text = sample_context_by_list(entityArr, text, CFG.max_len-len(entity_content))\n",
    "    inputs = TOKENIZER(text, entity_content, \n",
    "               add_special_tokens=True,\n",
    "               truncation = True,\n",
    "               max_length=CFG.max_len,\n",
    "               padding=\"max_length\",\n",
    "               return_offsets_mapping=False)\n",
    "\n",
    "    labels_ids = [0] * len(inputs.input_ids)\n",
    "    entityInfoItems = getTestEntityInfo(entityArr, TOKENIZER)\n",
    "    for entity, info in entityInfoItems:\n",
    "#         print(1111, entity, info)\n",
    "        entity_startIndexs = maxtch_token(info[0], inputs.input_ids)\n",
    "        labels_ids = tag_entity_span(entity_startIndexs, info[-1], info[1], labels_ids) # 标注labels_ids序列\n",
    "\n",
    "    assert len(inputs['input_ids']) == len(labels_ids)\n",
    "\n",
    "    # 转换为tensor\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    labels_ids = torch.tensor(labels_ids, dtype=torch.long)\n",
    "\n",
    "    return inputs, labels_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc428603",
   "metadata": {},
   "source": [
    "##  MaskSeqLabel Formater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31a30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_SL_formater(entityArr, text, TOKENIZER):\n",
    "    \n",
    "    text = sample_context_by_list(entityArr, text, CFG.max_len)\n",
    "    # 保证每个实体出现在文本中\n",
    "    text = '你对%s怎么看？' % '、'.join(entityArr) + text\n",
    "    \n",
    "    entitys = []\n",
    "    temp = {}\n",
    "    for i,entity in enumerate(entityArr):\n",
    "        key = '[et%d]' % i\n",
    "        entitys.append(key)\n",
    "        temp[entity] = len(entity)\n",
    "    temp = sorted(temp.items(), key=lambda x:-x[1]) # 实体按长度排序，避免长词包含短词的情况\n",
    "\n",
    "    for idx, item in enumerate(temp):\n",
    "        key = '[et%d]' % idx\n",
    "        text = text.replace(item[0], key) # 替换原实体\n",
    "        \n",
    "    inputs = TOKENIZER(text,\n",
    "               add_special_tokens=True,\n",
    "               truncation = True,\n",
    "               max_length=CFG.max_len,\n",
    "               padding=\"max_length\",\n",
    "               return_offsets_mapping=False)\n",
    "\n",
    "    idDic = {}  # label_id\n",
    "    label_ids = []\n",
    "    for idx, entity in enumerate(entitys):\n",
    "        idDic[TOKENIZER(entity).input_ids[1]] = idx+1\n",
    "    for each in inputs.input_ids:\n",
    "        if each in idDic:\n",
    "            label_ids.append(idDic[each])  \n",
    "        else:\n",
    "            label_ids.append(0) \n",
    "#     print(text)\n",
    "#     print(entitys)\n",
    "#     print(inputs.input_ids)\n",
    "#     en_cnt = 0\n",
    "#     for each in label_ids:\n",
    "#         if each :\n",
    "#             en_cnt += 1\n",
    "#     print(en_cnt, label_ids)\n",
    "    label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    \n",
    "    return inputs, label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51cd88",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4aa152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqLabeling(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = torch.load(config_path)\n",
    "        print(f'load from {config_path}')\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        self.drop1=nn.Dropout(0.1)\n",
    "        self.drop2=nn.Dropout(0.2)\n",
    "        self.drop3=nn.Dropout(0.3)\n",
    "        self.drop4=nn.Dropout(0.4)\n",
    "        self.drop5=nn.Dropout(0.5)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def loss(self,logits,labels,weights):\n",
    "        loss_fnc = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([0.1, 2, 1, 0.5, 1, 3])).float() ,\n",
    "                                        size_average=True,\n",
    "                                        reduction='none').cuda()\n",
    "        loss = loss_fnc(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, labels=None, weights=None, training=True):\n",
    "        feature = self.model(**inputs)[0]\n",
    "        if  training:\n",
    "            logits1 = self.fc(self.drop1(feature))\n",
    "            logits2 = self.fc(self.drop2(feature))\n",
    "            logits3 = self.fc(self.drop3(feature))\n",
    "            logits4 = self.fc(self.drop4(feature))\n",
    "            logits5 = self.fc(self.drop5(feature))\n",
    "            _loss=0\n",
    "            if labels is not None:\n",
    "                loss1 = self.loss(logits1.permute(0, 2, 1), labels, weights)\n",
    "                loss2 = self.loss(logits2.permute(0, 2, 1), labels, weights)\n",
    "                loss3 = self.loss(logits3.permute(0, 2, 1), labels, weights)\n",
    "                loss4 = self.loss(logits4.permute(0, 2, 1), labels, weights)\n",
    "                loss5 = self.loss(logits5.permute(0, 2, 1), labels, weights)\n",
    "                _loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n",
    "                # _loss = loss3\n",
    "            return _loss\n",
    "        else:\n",
    "            output = self.fc(feature)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53c911",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67dd79d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "ernie_tokenizer = AutoTokenizer.from_pretrained('./nlp_final_model/ernie-gram-zh-tokenizer/')\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('./nlp_final_model/chinese-roberta-base-tokenizer/')\n",
    "\n",
    "characters=[]\n",
    "for i in range(30):\n",
    "    characters.append('[et%d]' % i )\n",
    "add_ernie_tokenizer = AutoTokenizer.from_pretrained('./nlp_final_model/ernie-gram-zh-tokenizer/')\n",
    "add_ernie_tokenizer.add_tokens(characters)\n",
    "\n",
    "add_roberta_tokenizer = AutoTokenizer.from_pretrained('./nlp_final_model/chinese-roberta-base-tokenizer/')\n",
    "add_roberta_tokenizer.add_tokens(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b169bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from ./nlp_final_model/final_ernie_saved/config.pth\n",
      "load from ./nlp_final_model/final_roberta_epoch16_saved/config.pth\n",
      "load from ./nlp_final_model/mask_final_ernie_epoch8_saved/config.pth\n",
      "load from ./nlp_final_model/mask_final_roberta_epoch16_saved/config.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeqLabeling(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21158, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (drop2): Dropout(p=0.2, inplace=False)\n",
       "  (drop3): Dropout(p=0.3, inplace=False)\n",
       "  (drop4): Dropout(p=0.4, inplace=False)\n",
       "  (drop5): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model1 = SeqLabeling(CFG, config_path=CFG.seq_model1+'config.pth')\n",
    "seq_model1.to(DEVICE)\n",
    "model_path = os.path.join(CFG.seq_model1, f\"model_fold0_best.bin\")\n",
    "seq_model1.load_state_dict(torch.load(model_path,map_location=torch.device(device)))\n",
    "seq_model1.eval()\n",
    "\n",
    "seq_model2 = SeqLabeling(CFG, config_path=CFG.seq_model2+'config.pth')\n",
    "seq_model2.to(DEVICE)\n",
    "model_path = os.path.join(CFG.seq_model2, f\"model_fold0_best.bin\")\n",
    "seq_model2.load_state_dict(torch.load(model_path,map_location=torch.device(device)))\n",
    "seq_model2.eval()\n",
    "\n",
    "mask_model1 = SeqLabeling(CFG, config_path=CFG.mask_model1+'config.pth')\n",
    "mask_model1.to(DEVICE)\n",
    "model_path = os.path.join(CFG.mask_model1, f\"model_fold0_best.bin\")\n",
    "# mask_model1.resize_token_embeddings(len(add_ernie_tokenizer)) \n",
    "mask_model1.load_state_dict(torch.load(model_path,map_location=torch.device(device)))\n",
    "mask_model1.eval()\n",
    "\n",
    "\n",
    "mask_model2 = SeqLabeling(CFG, config_path=CFG.mask_model2+'config.pth')\n",
    "mask_model2.to(DEVICE)\n",
    "model_path = os.path.join(CFG.mask_model2, f\"model_fold0_best.bin\")\n",
    "# mask_model2.resize_token_embeddings(len(add_roberta_tokenizer)) \n",
    "mask_model2.load_state_dict(torch.load(model_path,map_location=torch.device(device)))\n",
    "mask_model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c1005",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b252702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb70e6c7840e4bdaa0dc03896e89f99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = []\n",
    "truth = []\n",
    "with open('./section1.txt', 'w') as fw:\n",
    "    with open(CFG.test_file, 'r') as f:\n",
    "#     with open('../nlp_data/newTest.txt', 'r') as f:\n",
    "#     with open('../Recommendation/data/rec_data/all_content.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            dic = json.loads(line.strip())\n",
    "            entityLen = len(dic['entity'])\n",
    "            if type(dic['entity']) == dict:\n",
    "                entityArr = list(dic['entity'].keys())\n",
    "                truth.extend(list(dic['entity'].values()))\n",
    "            elif type(dic['entity']) == list:\n",
    "                entityArr = dic['entity']\n",
    "            else:\n",
    "                print('type error!')\n",
    "            index = 0\n",
    "#             print(dic['entity'])\n",
    "            inputs1, label_ids1 = SL_formater(dic['entity'], dic['content'], ernie_tokenizer)\n",
    "            inputs2, label_ids2 = SL_formater(dic['entity'], dic['content'], roberta_tokenizer)\n",
    "            inputs3, label_ids3 = mask_SL_formater(dic['entity'], dic['content'], add_ernie_tokenizer)\n",
    "            inputs4, label_ids4 = mask_SL_formater(dic['entity'], dic['content'], add_roberta_tokenizer)\n",
    "            \n",
    "            for inputs in [inputs1,inputs2,inputs3,inputs4]:\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "                \n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    pred_logits1 = seq_model1(inputs1,training=False)\n",
    "                    pred_logits2 = seq_model2(inputs2,training=False)\n",
    "                    pred_logits3 = mask_model1(inputs3,training=False)\n",
    "                    pred_logits4 = mask_model2(inputs4,training=False)\n",
    "            \n",
    "            tmp_result = {}\n",
    "            model_logits = []\n",
    "            label_ids_arr = [label_ids1, label_ids2, label_ids3, label_ids4]\n",
    "            for idx, pred_logits in enumerate([pred_logits1, pred_logits2, pred_logits3, pred_logits4]):\n",
    "                pred_logits = pred_logits.squeeze(0).detach().cpu().numpy()\n",
    "                label_ids = label_ids_arr[idx]\n",
    "                \n",
    "#                 print(f'model{idx} label = ', end='')\n",
    "                temp = []  # [样本实体数, 5]\n",
    "                for i in range(1, max(label_ids)+1):\n",
    "                    ind = np.where(label_ids==i)\n",
    "                    logits = pred_logits[ind]   # [实体字数, 6] \n",
    "                    logits = logits[:,1: ]      # [实体字数, 5] \n",
    "                    logits = softmax(logits, axis=-1) \n",
    "                    logits = np.mean(logits, axis=0)\n",
    "                    temp.append(logits)\n",
    "#                     print(logits.argmax()-2, end=' ')\n",
    "#                 print()\n",
    "                model_logits.append(temp)\n",
    "            for i in range(len(model_logits[0])):\n",
    "                final_logits = 0.9*(0.5*model_logits[0][i]+0.5*model_logits[1][i])+0.1*(0.5*model_logits[2][i]+0.5*model_logits[3][i])\n",
    "                label = final_logits.argmax()-2\n",
    "                tmp_result[entityArr[i]] = label\n",
    "                pred.append(label)\n",
    "#             print(1111, str(dic['id']), tmp_result)\n",
    "            fw.write(str(dic['id']) + '\t' + json.dumps(tmp_result, ensure_ascii=False, cls=NpEncoder) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9e30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(truth, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e3543",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9823becd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>deviceType</th>\n",
       "      <th>logTs_gap</th>\n",
       "      <th>Hour</th>\n",
       "      <th>bm25_mean</th>\n",
       "      <th>co_bm25_mean</th>\n",
       "      <th>bm25</th>\n",
       "      <th>bm25Len</th>\n",
       "      <th>gapMeanEmotion</th>\n",
       "      <th>groupLen</th>\n",
       "      <th>historyHitSort</th>\n",
       "      <th>gongxian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>94</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   province  city  deviceType  logTs_gap  Hour  bm25_mean  co_bm25_mean  bm25  \\\n",
       "0         2     9           0          0    23          3             7     6   \n",
       "1         2     9           0          0    23         22             7    44   \n",
       "2         2     9           0          0    23         19             7    37   \n",
       "3         2     9           0          0    23         47             7    94   \n",
       "4         2     9           0          0    23         20             7    39   \n",
       "\n",
       "   bm25Len  gapMeanEmotion  groupLen  historyHitSort  gongxian  \n",
       "0       19              32        14               1         3  \n",
       "1       19              15        14               8        27  \n",
       "2       19              24        14              11        25  \n",
       "3       19              12        14               2         7  \n",
       "4       19               9        14               7         9  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTest_all_feature = pd.read_csv('./newTest_all_feature.csv')\n",
    "feature_names = ['province', 'city', 'deviceType', 'logTs_gap', 'Hour', 'bm25_mean', 'co_bm25_mean', \n",
    "                 'bm25', 'bm25Len', 'gapMeanEmotion', 'groupLen', 'historyHitSort', 'gongxian']\n",
    "test_model_input = {name: newTest_all_feature[name] for name in feature_names}\n",
    "newTest_all_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ccea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testSampleId</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.281802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.314887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.275916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.287720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000005</td>\n",
       "      <td>0.263973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405333</th>\n",
       "      <td>8405334</td>\n",
       "      <td>0.241028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405334</th>\n",
       "      <td>8405335</td>\n",
       "      <td>0.113012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405335</th>\n",
       "      <td>8405336</td>\n",
       "      <td>0.238159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405336</th>\n",
       "      <td>8405337</td>\n",
       "      <td>0.147870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405337</th>\n",
       "      <td>8405338</td>\n",
       "      <td>0.267649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1405338 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         testSampleId         0\n",
       "0             7000001  0.281802\n",
       "1             7000002  0.314887\n",
       "2             7000003  0.275916\n",
       "3             7000004  0.287720\n",
       "4             7000005  0.263973\n",
       "...               ...       ...\n",
       "1405333       8405334  0.241028\n",
       "1405334       8405335  0.113012\n",
       "1405335       8405336  0.238159\n",
       "1405336       8405337  0.147870\n",
       "1405337       8405338  0.267649\n",
       "\n",
       "[1405338 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(CFG.rec_model)\n",
    "testSampleId = pd.read_csv(CFG.rec_test)['testSampleId']\n",
    "pred_ans = model.predict(test_model_input, batch_size=1024)\n",
    "pred = pd.Series(pred_ans.flatten('F'))\n",
    "output_df = pd.concat([testSampleId, pred], axis=1)\n",
    "output_path = os.path.join('./', \"section2.txt\")\n",
    "output_df.to_csv(output_path, index=False,sep=\"\\t\", header=[\"Id\", \"result\"])\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac998f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepctr",
   "language": "python",
   "name": "deepctr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
